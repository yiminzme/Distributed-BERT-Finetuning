{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb40d541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo mongod --dbpath ~/data/mongodb\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49582617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, length, lit\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
    "import os\n",
    "import uuid\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e796ed78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark with MongoDB connector\n",
    "def init_spark():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Distributed BERT Fine-Tuning\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/sentiment_db.reviews\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/sentiment_db.reviews\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Load IMDB and SST-2 data to MongoDB\n",
    "def load_data_to_mongodb(spark):\n",
    "    # IMDB dataset\n",
    "    imdb_dataset = load_dataset(\"imdb\")\n",
    "    imdb_df = pd.concat([\n",
    "        imdb_dataset[\"train\"].to_pandas()[[\"text\", \"label\"]].head(50), # use 50 for debug\n",
    "        imdb_dataset[\"test\"].to_pandas()[[\"text\", \"label\"]].head(50)\n",
    "    ])\n",
    "    imdb_df[\"source\"] = \"IMDB\"\n",
    "    imdb_spark_df = spark.createDataFrame(imdb_df).select(col(\"text\"), col(\"label\").cast(\"integer\"), col(\"source\"))\n",
    "    \n",
    "    # SST-2 dataset\n",
    "    sst2_dataset = load_dataset(\"glue\", \"sst2\")\n",
    "    sst2_df = sst2_dataset[\"train\"].to_pandas()[[\"sentence\", \"label\"]].head(50) # use 50 for debug\n",
    "    sst2_df = sst2_df.rename(columns={\"sentence\": \"text\"})\n",
    "    sst2_df[\"source\"] = \"SST-2\"\n",
    "    sst2_spark_df = spark.createDataFrame(sst2_df).select(col(\"text\"), col(\"label\").cast(\"integer\"), col(\"source\"))\n",
    "    \n",
    "    # Write to MongoDB\n",
    "    imdb_spark_df.write.format(\"mongo\").mode(\"append\").save()\n",
    "    sst2_spark_df.write.format(\"mongo\").mode(\"append\").save()\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_data(spark):\n",
    "    raw_df = spark.read.format(\"mongo\").load()\n",
    "    processed_df = raw_df.filter(length(col(\"text\")) >= 10)\n",
    "    \n",
    "    # Split IMDB into train/test\n",
    "    imdb_df = processed_df.filter(col(\"source\") == \"IMDB\")\n",
    "    train_df, test_df = imdb_df.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    # SST-2 as test set\n",
    "    sst2_test_df = processed_df.filter(col(\"source\") == \"SST-2\")\n",
    "    \n",
    "    # Store processed data in MongoDB\n",
    "    train_collection = f\"train_{uuid.uuid4().hex}\"\n",
    "    test_collection = f\"test_{uuid.uuid4().hex}\"\n",
    "    sst2_collection = f\"sst2_{uuid.uuid4().hex}\"\n",
    "    train_df.write.format(\"mongo\").option(\"collection\", train_collection).mode(\"overwrite\").save()\n",
    "    test_df.write.format(\"mongo\").option(\"collection\", test_collection).mode(\"overwrite\").save()\n",
    "    sst2_test_df.write.format(\"mongo\").option(\"collection\", sst2_collection).mode(\"overwrite\").save()\n",
    "    \n",
    "    return spark.read.format(\"mongo\").option(\"collection\", train_collection).load(), \\\n",
    "           spark.read.format(\"mongo\").option(\"collection\", test_collection).load(), \\\n",
    "           spark.read.format(\"mongo\").option(\"collection\", sst2_collection).load()\n",
    "\n",
    "# PyTorch Dataset for Spark DataFrame\n",
    "class SparkDataset(Dataset):\n",
    "    def __init__(self, spark_df, tokenizer, max_length=128):\n",
    "        self.data = spark_df.toPandas()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx][\"text\"]\n",
    "        label = self.data.iloc[idx][\"label\"]\n",
    "        encoding = self.tokenizer(text, padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Training and evaluation\n",
    "def train_and_evaluate(rank, world_size, train_dataset, test_dataset, sst2_test_dataset, batch_size=8, epochs=3):\n",
    "    # Setup distributed training\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"12345\"\n",
    "    dist.init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "    \n",
    "    # Initialize BERT\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "    model = DDP(model.to(rank), device_ids=[rank])\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    sst2_test_loader = DataLoader(sst2_test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Train\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(rank)\n",
    "            attention_mask = batch[\"attention_mask\"].to(rank)\n",
    "            labels = batch[\"labels\"].to(rank)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        if rank == 0:\n",
    "            print(f\"Epoch {epoch+1}, Avg Loss: {total_loss / len(train_loader):.4f}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    for dataset_name, loader in [(\"IMDB Test\", test_loader), (\"SST-2 Test\", sst2_test_loader)]:\n",
    "        correct = total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                input_ids = batch[\"input_ids\"].to(rank)\n",
    "                attention_mask = batch[\"attention_mask\"].to(rank)\n",
    "                labels = batch[\"labels\"].to(rank)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "                correct += (predictions == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        if rank == 0:\n",
    "            print(f\"{dataset_name} Accuracy: {correct / total:.4f}\")\n",
    "    \n",
    "    dist.destroy_process_group()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f9e93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5003",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
