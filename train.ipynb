{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c0607b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, length\n",
    "from pyspark.sql.types import ArrayType, IntegerType, StructType, StructField\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import os\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import pyarrow.parquet as pq\n",
    "import glob\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ed04ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/25 19:14:17 WARN Utils: Your hostname, yPC resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/04/25 19:14:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/goodh/.ivy2/cache\n",
      "The jars for the packages stored in: /home/goodh/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d5fe131a-ac5d-4d22-9abd-81c213a111bf;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 77ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d5fe131a-ac5d-4d22-9abd-81c213a111bf\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/3ms)\n",
      "25/04/25 19:14:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Distributed BERT Fine-Tuning with Preprocessing\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.cores.max\", 4) \\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/sentiment_db.reviews\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/sentiment_db.reviews\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5532ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "imdb_df = pd.concat([\n",
    "    imdb_dataset['train'].to_pandas()[['text', 'label']],\n",
    "    imdb_dataset['test'].to_pandas()[['text', 'label']],\n",
    "]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "imdb_df['source'] = 'IMDB'\n",
    "\n",
    "sst2_dataset = load_dataset('glue', 'sst2')\n",
    "sst2_df = sst2_dataset['train'].to_pandas()[['sentence', 'label']].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "sst2_df = sst2_df.rename(columns={'sentence': 'text'})\n",
    "sst2_df['source'] = 'SST-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fa8cbc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_spark_df = spark.createDataFrame(imdb_df).select(col('text'), col('label').cast('integer'), col('source'))\n",
    "sst2_spark_df = spark.createDataFrame(sst2_df).select(col('text'), col('label').cast('integer'), col('source'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af27203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_tokenizer_udf(max_length=128):\n",
    "    def tokenize_batch(texts: pd.Series) -> pd.DataFrame:\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        encodings = tokenizer(\n",
    "            texts.tolist(),\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"np\"\n",
    "        )\n",
    "        return pd.DataFrame({\n",
    "            \"input_ids\": [ids.tolist() for ids in encodings[\"input_ids\"]],\n",
    "            \"attention_mask\": [mask.tolist() for mask in encodings[\"attention_mask\"]]\n",
    "        })\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"input_ids\", ArrayType(IntegerType())),\n",
    "        StructField(\"attention_mask\", ArrayType(IntegerType()))\n",
    "    ])\n",
    "    \n",
    "    return pandas_udf(tokenize_batch, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0f6ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = imdb_spark_df.filter(length(col('text'))>=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac98a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_udf = create_batch_tokenizer_udf(128)\n",
    "tokenized_df = processed_df.withColumn('encoding', tokenize_udf(col('text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b05b37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df = tokenized_df.select(\n",
    "    col(\"label\").cast(\"integer\").alias(\"label\"),\n",
    "    col(\"source\"),\n",
    "    col(\"encoding.input_ids\").alias(\"input_ids\"),\n",
    "    col(\"encoding.attention_mask\").alias(\"attention_mask\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "211b82d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spark_df, test_spark_df = imdb_spark_df.randomSplit([0.8, 0.2], seed=42)\n",
    "sst2_spark_test_df = sst2_spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b4506b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6795020675783696,\n",
       " 0.6675350687243516,\n",
       " 0.6599975805975139,\n",
       " 0.6501290568721974,\n",
       " 0.6425037231352815,\n",
       " 0.6386665645982164,\n",
       " 0.6346162742448962,\n",
       " 0.6312412393831311,\n",
       " 0.6338303024067389,\n",
       " 0.6215268512384063,\n",
       " 0.6137265110306407,\n",
       " 0.6172930431199946,\n",
       " 0.6118318956605655,\n",
       " 0.6112569093229877,\n",
       " 0.6062058276781482,\n",
       " 0.6037738823795816,\n",
       " 0.6018424541966235,\n",
       " 0.5990889155725376,\n",
       " 0.6015712148927984,\n",
       " 0.5980112003289891]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['sst2_eval_losses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0a014ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scaler_state_dict', 'train_losses', 'imdb_eval_losses', 'sst2_eval_losses'])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5622679c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2, hidden_dropout_prob=0.3, attention_probs_dropout_prob=0.3)\n",
    "optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=2e-5)\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "checkpoint = torch.load(\"./checkpoints/20250425_210603_bert_finetuned_epoch_20.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "scaler.load_state_dict(checkpoint['scaler_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db3a163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyParquetDataset(IterableDataset):\n",
    "    def __init__(self, parquet_path, rank, world_size, batch_size=1000):\n",
    "        self.parquet_files = sorted(glob.glob(os.path.join(parquet_path, \"*.parquet\")))\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Shard files across ranks\n",
    "        files_per_rank = len(self.parquet_files) // world_size\n",
    "        start_idx = rank * files_per_rank\n",
    "        end_idx = (rank + 1) * files_per_rank if rank < world_size - 1 else len(self.parquet_files)\n",
    "        self.parquet_files = self.parquet_files[start_idx:end_idx]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for file in self.parquet_files:\n",
    "            # print(f\"Rank {self.rank} reading Parquet file: {file}\")\n",
    "            parquet_file = pq.ParquetFile(file)\n",
    "            for batch in parquet_file.iter_batches(batch_size=self.batch_size):\n",
    "                df = batch.to_pandas()\n",
    "                for _, row in df.iterrows():\n",
    "                    yield {\n",
    "                        \"input_ids\": torch.tensor(row[\"input_ids\"], dtype=torch.long),\n",
    "                        \"attention_mask\": torch.tensor(row[\"attention_mask\"], dtype=torch.long),\n",
    "                        \"labels\": torch.tensor(row[\"label\"], dtype=torch.long)\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "939cd407",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LazyParquetDataset('archive/train/', 0, 1)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05424e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "81d548de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 0, 0, 1, 1, 0] [True, False, True, True, True, False, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 1] [True, True, True, True, True, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 0, 1, 0, 0, 1] [False, False, True, True, False, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 0, 0, 0, 1, 0] [False, False, True, True, True, True, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 0, 0, 0, 0, 0] [False, False, False, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 1, 1, 0, 0, 0, 1] [True, False, False, False, True, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 0, 1, 0, 0, 0] [False, False, False, True, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 0, 0, 0, 0, 0] [False, True, False, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 1, 1, 0, 0] [True, True, False, True, False, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 0, 1, 0, 0, 0] [False, False, True, True, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 0, 1, 0, 1, 0] [False, True, False, True, False, True, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 0, 0, 1, 0] [True, True, False, True, True, True, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 0, 0, 0, 0] [True, True, False, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 0, 1, 1, 1, 0] [False, True, False, True, False, False, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 1, 0, 1, 0, 1] [False, False, True, False, True, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 0, 1, 1, 0, 1] [False, True, False, True, False, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 1, 0, 1, 1] [True, True, True, True, False, True, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 1, 1, 0, 0, 0] [False, True, False, False, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 0, 0, 0, 1] [True, True, False, True, True, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 1, 0, 0, 1] [True, True, False, True, False, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 1, 1, 0, 0] [True, True, False, True, False, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 1, 0, 0] [True, True, True, True, True, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 0, 0, 1, 1, 0] [False, False, True, True, True, False, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 1, 1, 1, 1, 0] [False, False, True, False, False, False, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 1, 0, 0, 0, 0] [False, False, True, False, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 0, 1, 0, 0, 1] [False, False, False, True, False, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 0, 0, 0] [False, False, False, False, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 1, 0, 0, 0, 0, 1] [True, False, False, True, True, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 0, 0, 0, 0, 1] [False, False, False, True, True, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 0, 0, 0, 1, 1] [False, False, True, True, True, True, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 1, 0, 1, 0, 1] [False, True, True, False, True, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 1, 0] [True, True, True, True, True, True, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 0, 0, 0, 0, 1] [False, True, False, True, True, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 0, 1, 1, 0, 1] [True, False, True, True, False, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 1, 0, 1, 0, 0] [True, True, False, False, True, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 0, 1, 0, 0, 1] [True, False, True, True, False, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 0, 0, 0, 1, 0] [True, False, True, True, True, True, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 0, 1, 1, 1] [True, True, False, True, True, False, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 0, 0, 0, 0] [False, True, True, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 1, 1, 0, 0, 0] [False, True, True, False, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 0, 0, 0, 1, 0] [False, False, False, True, True, True, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 1, 1, 1, 0, 0] [False, True, False, False, False, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 1, 1, 1, 0, 0] [True, True, True, False, False, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 1, 0, 0, 0] [False, True, True, True, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 1, 0, 1, 0, 0] [False, True, True, False, True, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 1, 1, 0, 0] [True, True, False, True, False, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 1, 1, 1, 0, 0] [True, True, False, False, False, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 0, 0, 0, 1] [False, True, True, True, True, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 1, 1, 0, 0, 1] [False, True, True, False, False, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 1, 1, 0, 1, 0] [True, False, True, False, False, True, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 0, 0, 0, 0, 0] [True, False, True, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 1, 0, 1, 0, 1] [False, False, True, False, True, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 1, 0, 0, 0] [False, True, True, True, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 0, 0, 0, 0] [False, True, True, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 0, 0, 1, 0, 0] [True, False, True, True, True, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 1, 1, 0, 1, 0, 0] [True, False, False, False, True, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 0, 1, 0] [False, False, False, False, False, True, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 0, 1, 1, 0, 1] [False, False, True, True, False, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 1, 0, 0, 0] [False, True, True, True, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 1] [True, True, True, True, True, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 1, 1, 0, 0, 0, 0] [True, False, False, False, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 1, 0, 1, 0, 1] [True, True, True, False, True, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 1, 0, 1, 0, 1] [True, True, False, False, True, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 1, 1, 0, 0] [False, True, True, True, False, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 1, 1, 0, 0, 0] [True, True, True, False, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 1, 0, 1, 0, 1] [False, True, False, False, True, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 0, 0, 1, 0, 0] [False, False, True, True, True, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 0, 1, 1, 0, 1] [False, False, True, True, False, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 1, 1, 1] [True, True, True, True, True, False, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 1, 0, 0, 1, 0, 1] [True, False, False, True, True, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 1, 1, 1] [True, True, True, True, True, False, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 0, 1, 1, 1] [False, False, False, False, True, False, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 1, 0, 1] [True, True, True, True, True, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 1, 1, 0, 0, 0] [False, True, True, False, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 1, 1, 0, 0, 0, 0] [True, False, False, False, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 1, 0, 0, 1, 0] [True, False, True, False, True, True, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 1, 0, 0, 0] [True, True, True, True, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 0, 0, 0, 0, 0] [False, True, False, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 0, 0, 0, 0] [False, True, True, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 0, 0, 0, 0, 1] [False, False, False, True, True, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 0, 0, 1, 0, 1] [False, True, False, True, True, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 1, 1, 1, 1] [False, True, True, True, False, False, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 1, 1, 1, 0, 0] [True, True, True, False, False, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 1, 1, 0, 0] [True, True, False, True, False, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 0, 0, 0, 0] [False, True, True, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 1, 0, 1, 1] [True, True, True, True, False, True, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 1, 0, 1, 0, 0, 0] [True, False, False, True, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 0, 1, 1, 0, 0] [True, False, True, True, False, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 1, 0, 1, 0, 0] [True, True, False, False, True, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 1, 0, 1, 0] [True, True, True, True, False, True, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 1, 0, 0, 1] [True, True, True, True, False, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 1, 0, 0] [True, True, True, True, True, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 0, 0, 1, 0] [True, True, False, True, True, True, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 0, 0, 1] [False, False, False, False, False, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 0, 0, 1, 0, 0] [True, False, True, True, True, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 1, 1, 0, 0, 0] [True, False, True, False, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 1, 0, 1, 1, 0, 1] [True, False, False, True, False, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 1, 0, 0, 0] [False, True, True, True, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 0, 1, 1, 0, 1] [False, False, False, True, False, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 1, 0, 1, 0, 0] [True, False, True, False, True, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 0, 0, 1, 0, 0] [False, False, True, True, True, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 1, 1, 0, 1, 1] [False, True, False, False, False, True, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 1, 0, 1, 1, 0] [False, True, True, False, True, False, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 1, 1] [True, True, True, True, True, True, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 1, 0, 1, 0, 0] [False, False, True, False, True, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 1, 1, 0, 0, 1] [True, True, False, False, False, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 1, 0, 0, 0, 0] [False, True, True, False, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 1, 0, 0, 1, 1] [False, True, True, False, True, True, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 0, 0, 0, 1, 0] [True, False, True, True, True, True, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 1, 1, 0, 1, 1] [True, True, True, False, False, True, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 1, 0, 0, 0] [False, True, True, True, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 0, 1, 0, 1] [False, True, True, True, True, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 1, 0, 0, 0] [True, True, True, True, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 0, 1, 1, 1, 0] [True, False, True, True, False, False, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 1, 0, 0, 0, 0] [False, True, False, False, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 0, 0, 1, 1, 0] [False, True, False, True, True, False, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 0, 0, 1, 1, 1] [True, False, True, True, True, False, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 0, 0, 0, 0] [True, True, False, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 1, 0, 1, 1] [True, True, True, True, False, True, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 0, 0, 1, 0] [False, False, False, False, True, True, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 1, 1, 1, 1, 1, 0] [True, False, False, False, False, False, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 0, 0, 0, 0] [False, True, True, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 1, 0, 1, 0, 1] [True, True, True, False, True, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 1, 1, 1, 0] [False, True, True, True, False, False, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 0, 0, 1, 0, 1] [False, False, True, True, True, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 1, 0, 0, 0] [True, True, True, True, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 0, 0, 0, 1] [False, True, True, True, True, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 1, 0, 0, 0, 0, 0] [True, False, False, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 0, 0, 0, 1, 1] [True, False, True, True, True, True, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 1, 0, 0, 0, 0] [False, True, True, False, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 1, 0, 1, 0, 1] [True, False, True, False, True, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 0, 1, 1, 0] [True, True, False, True, True, False, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 1] [True, True, True, True, True, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 1, 0, 0, 0, 1] [False, True, False, False, True, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0] [True, True, True, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 0, 0, 1, 1, 0] [True, False, True, True, True, False, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 1, 0, 0, 1, 0] [True, False, True, False, True, True, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 0, 1, 0, 0, 0] [False, False, True, True, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 1, 1, 0, 1, 1] [True, True, True, False, False, True, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 0, 0, 0, 0] [True, True, False, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 0, 1, 0, 0, 0] [False, False, False, True, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 1, 0, 1, 1, 0] [False, True, True, False, True, False, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 0, 0, 1, 1, 0] [False, True, False, True, True, False, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0] [True, True, True, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 1, 0, 0] [True, True, True, True, True, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 1, 1, 1, 0, 0] [False, False, True, False, False, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 1, 1] [True, True, True, True, True, True, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 0, 0, 0, 0] [False, True, True, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 1, 1, 1, 0, 0] [False, True, True, False, False, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 1, 0, 0, 0] [True, True, True, True, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 1, 1, 0, 0] [False, True, True, True, False, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 1, 0, 1, 1, 0] [False, False, True, False, True, False, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 0, 0, 1, 0] [True, True, False, True, True, True, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 1, 0, 1, 0, 1] [True, True, True, False, True, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 0, 1, 1, 0] [False, True, True, True, True, False, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 0, 1, 1, 1, 1] [False, True, False, True, False, False, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 1, 0, 0, 0] [True, True, False, True, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 0, 0, 0, 0, 0] [False, False, True, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 0, 0, 1, 0, 1] [False, True, False, True, True, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 1, 0, 0, 1, 1] [False, True, False, False, True, True, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 0, 0, 0, 0] [True, True, False, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 1, 1, 1, 0, 0] [True, False, True, False, False, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 1, 0, 0, 0] [False, True, True, True, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 1, 0, 0, 0, 0] [True, False, True, False, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 1, 0, 0] [True, True, True, True, True, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 1, 0, 1, 0, 1] [True, True, True, False, True, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 0, 1, 1, 0] [True, True, False, True, True, False, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 0, 0, 0, 0, 0] [False, True, False, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 0, 0, 0, 0] [False, True, True, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 1, 0, 1, 0, 1] [False, True, True, False, True, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 0, 1, 1, 0] [True, True, False, True, True, False, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 1, 0, 0, 1, 0, 0] [True, False, False, True, True, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 1, 1, 0, 0] [False, True, True, True, False, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 0, 0, 0, 1, 0] [False, False, True, True, True, True, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 1, 1, 0, 0, 1] [True, False, True, False, False, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 1, 0, 0, 0, 0] [False, True, False, False, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 1, 1, 1, 0] [False, True, True, True, False, False, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 0, 1, 1, 0, 0] [False, False, False, True, False, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 0, 1, 0, 0, 0] [False, True, False, True, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 1, 0, 1, 1, 0] [False, False, True, False, True, False, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 0, 0, 0, 0, 0] [False, True, False, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 1, 1, 1, 0, 1] [True, True, True, False, False, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 1, 0, 0, 0, 1] [True, True, True, False, True, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 1, 1] [True, True, True, True, True, True, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 1, 1, 1, 0, 1] [True, True, True, False, False, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 0, 1, 1, 0] [False, True, True, True, True, False, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 0, 1, 0, 0, 1] [True, False, True, True, False, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 0, 1, 0, 0, 0] [True, False, True, True, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 1, 0, 0, 1, 0] [False, True, True, False, True, True, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 0, 0, 0, 1] [True, True, False, True, True, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 1, 1, 0, 0, 1] [True, True, True, False, False, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 0, 0, 0, 0] [False, True, True, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 1, 0, 0, 1, 1] [True, False, True, False, True, True, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 0, 0, 0, 1] [True, True, False, True, True, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 0, 0, 1, 0, 0] [False, False, True, True, True, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 1, 1, 1, 1, 0, 0] [True, False, False, False, False, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 1, 0, 0, 1, 1] [False, True, False, False, True, True, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 1] [True, True, True, True, True, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 1] [True, True, True, True, True, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 1, 0, 1] [True, True, True, True, True, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 1, 0, 0, 1] [True, True, False, True, False, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 1, 0, 0, 0, 0] [True, True, True, False, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 0, 0, 0, 0, 1] [True, False, True, True, True, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 0, 1, 0, 1, 0] [False, False, False, True, False, True, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 1, 0, 1, 0] [False, True, True, True, False, True, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0] [True, True, True, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0] [True, True, True, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 1, 0, 0, 1, 0, 1] [True, False, False, True, True, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 1, 1, 0, 1] [False, True, True, True, False, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 0, 0, 0, 0] [True, True, False, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 1, 1, 1, 0, 0] [False, True, True, False, False, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 1, 0, 0, 0, 0] [False, True, False, False, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 0, 0, 0, 1, 1] [False, False, True, True, True, True, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 1, 0, 0, 0, 1, 1] [True, False, False, True, True, True, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 0, 0, 0, 0] [True, True, False, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 1, 1, 1, 1, 0] [True, True, True, False, False, False, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 1, 0, 0, 1] [True, True, True, True, False, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 1, 1, 0, 0, 1] [True, False, True, False, False, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 0, 0, 0, 0] [True, True, False, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 1, 1, 0, 0, 1] [True, True, False, False, False, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 1, 0, 0, 0, 1, 0] [True, False, False, True, True, True, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 1, 0, 1, 0, 0, 0] [True, False, False, True, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 0, 1, 0, 1, 1] [False, False, True, True, False, True, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 1, 0, 1, 0, 0, 1] [True, False, False, True, False, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 1, 1, 1, 0, 0] [True, True, True, False, False, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 1, 0, 1, 0, 0] [True, True, True, False, True, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 1, 1, 1, 0, 0] [False, True, False, False, False, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 0, 0, 0, 0] [True, True, False, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 0, 0, 0, 0] [False, True, True, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 0, 1, 0, 0, 0] [False, False, True, True, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 1, 1, 0, 0, 0] [True, False, True, False, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 0, 0, 0, 1, 1] [True, False, True, True, True, True, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 1, 0, 0, 1, 1] [True, True, True, False, True, True, False, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 1, 1, 0, 0] [True, True, True, True, False, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 1, 1, 0, 1] [False, True, True, True, False, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 1, 0, 0] [True, True, True, True, True, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 1, 0, 0, 1, 0, 0] [False, True, False, True, True, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 1, 1, 0, 1, 1, 0] [True, False, False, False, True, False, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 1, 1, 0, 0, 1] [False, False, True, False, False, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 1, 0, 0, 0, 0] [False, False, True, False, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 1] [True, True, True, True, True, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 0, 0] [False, False, False, False, False, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 1, 0, 0, 0, 0, 1] [True, False, False, True, True, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 1, 1, 0, 1] [True, True, True, True, False, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 1, 1, 1, 0, 1] [True, False, True, False, False, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 0, 0, 0, 1] [False, True, True, True, True, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 1] [True, True, True, True, True, True, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 1, 1, 0, 0, 0] [True, True, True, False, False, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 0, 0, 1, 0] [True, True, False, True, True, True, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 1, 0, 0, 0, 0, 0] [True, False, False, True, True, True, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 1, 0, 1, 1, 0, 1] [True, False, False, True, False, False, True, False]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 1, 1, 1, 0, 0] [True, True, True, False, False, False, True, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 0, 0, 0, 1, 0] [True, False, True, True, True, True, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 1, 1, 0] [True, True, True, True, True, False, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 0, 0, 0, 0, 0, 1, 0] [False, True, True, True, True, True, False, True]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 0, 0, 1, 0, 1, 0] [False, False, True, True, False, True, False, True]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "correct = total = count = 0\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['labels'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        print(predictions.tolist(), labels.tolist(), (predictions==labels).tolist())\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        count += predictions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be03acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1647d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710c5d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a84978b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acf9f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a0e97e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing Spark...\n",
      "INFO:__main__:4 cores for spark\n",
      "25/04/22 23:28:22 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "INFO:__main__:No cached Parquet files found. Running full pipeline...\n",
      "INFO:__main__:Loading data to MongoDB...\n",
      "INFO:__main__:Loading IMDB dataset...\n",
      "INFO:__main__:Loading SST-2 dataset...\n",
      "INFO:__main__:Writing datasets to MongoDB...\n",
      "25/04/22 23:28:59 WARN TaskSetManager: Stage 5 contains a task of very large size (1327 KiB). The maximum recommended task size is 1000 KiB.\n",
      "INFO:__main__:Data loading to MongoDB took 38.81 seconds                        \n",
      "INFO:__main__:Distributed preprocessing and saving to Parquet...\n",
      "INFO:__main__:Reading data from MongoDB...\n",
      "INFO:__main__:Tokenizing data...\n",
      "INFO:__main__:num_partitions 56\n",
      "INFO:__main__:Writing Parquet files: train=processed_data/train_5933d11a44e643b297ab8a2a7b33a7ad, test=processed_data/test_3d891be5c6144acfb2990326693eaf61, sst2=processed_data/sst2_0d099f233d3e4bdda9705d267d36ed62\n",
      "25/04/22 23:29:01 WARN TaskSetManager: Stage 7 contains a task of very large size (1327 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/22 23:29:21 WARN MemoryManager: Total allocation exceeds 95.00% (3,626,971,910 bytes) of heap memory\n",
      "Scaling row group sizes to 96.51% for 28 writers\n",
      "INFO:__main__:21.0736s for train_df partition                                   \n",
      "INFO:__main__:Distributed preprocessing took 21.15 seconds\n",
      "INFO:__main__:Using 1 GPU(s)\n",
      "INFO:__main__:Distributed fine-tuning...\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'train_and_evaluate' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "ProcessExitedException",
     "evalue": "process 0 terminated with exit code 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 321\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultiprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[1;32m    320\u001b[0m finetune_time \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(world_size, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mshare_memory_()\n\u001b[0;32m--> 321\u001b[0m \u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msst2_test_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_collection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_collection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msst2_collection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinetune_time\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    326\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# append results\u001b[39;00m\n\u001b[1;32m    329\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mNUM_CPUs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mNUM_GPUs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpreprocess_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfinetune_time[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/5003/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:340\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    334\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis method only supports start_method=spawn (got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use a different start_method use:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    337\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n\u001b[1;32m    339\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdaemon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspawn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/5003/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:296\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/5003/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:204\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout, grace_period)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    197\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with signal \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (error_index, name),\n\u001b[1;32m    198\u001b[0m             error_index\u001b[38;5;241m=\u001b[39merror_index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m             signal_name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    202\u001b[0m         )\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    205\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with exit code \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (error_index, exitcode),\n\u001b[1;32m    206\u001b[0m             error_index\u001b[38;5;241m=\u001b[39merror_index,\n\u001b[1;32m    207\u001b[0m             error_pid\u001b[38;5;241m=\u001b[39mfailed_process\u001b[38;5;241m.\u001b[39mpid,\n\u001b[1;32m    208\u001b[0m             exit_code\u001b[38;5;241m=\u001b[39mexitcode,\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_files[error_index], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh:\n\u001b[1;32m    212\u001b[0m     original_trace \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(fh)\n",
      "\u001b[0;31mProcessExitedException\u001b[0m: process 0 terminated with exit code 1"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, length\n",
    "from pyspark.sql.types import ArrayType, IntegerType, StructType, StructField\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import os\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import pyarrow.parquet as pq\n",
    "import glob\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Spark with MongoDB connector\n",
    "def init_spark(num_cpus = None):\n",
    "    # if num_spark_executor_core: logger.info(f\"{num_spark_executor_core} cores for executor\")\n",
    "    # else: logger.info(f\"number of cores for executor UNDEFINED\")\n",
    "    if num_cpus: logger.info(f\"{num_cpus} cores for spark\")\n",
    "    else: logger.info(f\"num_cpus UNDEFINED\")\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Distributed BERT Fine-Tuning with Preprocessing\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.cores.max\", num_cpus) \\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/sentiment_db.reviews\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/sentiment_db.reviews\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "        .config(\"spark.mongodb.input.partitionerOptions.partitionSizeMB\", \"256\") \\\n",
    "        .getOrCreate()\n",
    "        # .config(\"spark.driver.cores\", \"2\") \\\n",
    "        # .config(\"spark.executor.cores\", str(num_spark_executor_core) if num_spark_executor_core else 4) \\\n",
    "        # .config(\"spark.executor.instances\", 3) \\\n",
    "        # .config(\"spark.default.parallelism\", 10) \\\n",
    "    return spark\n",
    "\n",
    "# Load IMDB and SST-2 data to MongoDB\n",
    "def load_data_to_mongodb(spark):\n",
    "    # IMDB dataset\n",
    "    start_time = time.time()\n",
    "    logger.info(\"Loading IMDB dataset...\")\n",
    "    imdb_dataset = load_dataset(\"imdb\")\n",
    "    imdb_df = pd.concat([\n",
    "        imdb_dataset[\"train\"].to_pandas()[[\"text\", \"label\"]],  # use 50 for debug\n",
    "        imdb_dataset[\"test\"].to_pandas()[[\"text\", \"label\"]]\n",
    "    ])\n",
    "    imdb_df[\"source\"] = \"IMDB\"\n",
    "    imdb_spark_df = spark.createDataFrame(imdb_df).select(col(\"text\"), col(\"label\").cast(\"integer\"), col(\"source\"))\n",
    "    \n",
    "    # SST-2 dataset\n",
    "    logger.info(\"Loading SST-2 dataset...\")\n",
    "    sst2_dataset = load_dataset(\"glue\", \"sst2\")\n",
    "    sst2_df = sst2_dataset[\"train\"].to_pandas()[[\"sentence\", \"label\"]]  # use 50 for debug\n",
    "    sst2_df = sst2_df.rename(columns={\"sentence\": \"text\"})\n",
    "    sst2_df[\"source\"] = \"SST-2\"\n",
    "    sst2_spark_df = spark.createDataFrame(sst2_df).select(col(\"text\"), col(\"label\").cast(\"integer\"), col(\"source\"))\n",
    "    \n",
    "    logger.info(\"Writing datasets to MongoDB...\")\n",
    "    imdb_spark_df.write.format(\"mongo\").mode(\"append\").save()\n",
    "    sst2_spark_df.write.format(\"mongo\").mode(\"append\").save()\n",
    "    return time.time() - start_time, imdb_spark_df, sst2_spark_df\n",
    "\n",
    "# Batch tokenizer UDF\n",
    "def create_batch_tokenizer_udf(max_length=128):\n",
    "    def tokenize_batch(texts: pd.Series) -> pd.DataFrame:\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        encodings = tokenizer(\n",
    "            texts.tolist(),\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"np\"\n",
    "        )\n",
    "        return pd.DataFrame({\n",
    "            \"input_ids\": [ids.tolist() for ids in encodings[\"input_ids\"]],\n",
    "            \"attention_mask\": [mask.tolist() for mask in encodings[\"attention_mask\"]]\n",
    "        })\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"input_ids\", ArrayType(IntegerType())),\n",
    "        StructField(\"attention_mask\", ArrayType(IntegerType()))\n",
    "    ])\n",
    "    \n",
    "    return pandas_udf(tokenize_batch, schema)\n",
    "\n",
    "# Preprocess data and save to Parquet\n",
    "def preprocess_data(spark, imdb_spark_df, sst2_spark_df, output_dir, max_length=128):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load and preprocess data\n",
    "    logger.info(\"Reading data from MongoDB...\")\n",
    "    # raw_df = spark.read.format(\"mongo\").load()\n",
    "    raw_df = imdb_spark_df.union(sst2_spark_df)\n",
    "    processed_df = raw_df.filter(length(col(\"text\")) >= 10)\n",
    "    \n",
    "    # Apply distributed batch tokenization\n",
    "    logger.info(\"Tokenizing data...\")\n",
    "    tokenize_udf = create_batch_tokenizer_udf(max_length)\n",
    "    tokenized_df = processed_df.withColumn(\"encoding\", tokenize_udf(col(\"text\")))\n",
    "    \n",
    "    # Extract input_ids and attention_mask\n",
    "    tokenized_df = tokenized_df.select(\n",
    "        col(\"label\").cast(\"integer\").alias(\"label\"),\n",
    "        col(\"source\"),\n",
    "        col(\"encoding.input_ids\").alias(\"input_ids\"),\n",
    "        col(\"encoding.attention_mask\").alias(\"attention_mask\")\n",
    "    )\n",
    "    \n",
    "    # Split IMDB into train/test\n",
    "    imdb_df = tokenized_df.filter(col(\"source\") == \"IMDB\")\n",
    "    train_df, test_df = imdb_df.randomSplit([0.8, 0.2], seed=42)\n",
    "    sst2_test_df = tokenized_df.filter(col(\"source\") == \"SST-2\")\n",
    "    \n",
    "    # Save to Parquet with dynamic partitioning\n",
    "    num_partitions = max(16, spark.sparkContext.defaultParallelism * 2)  # Adjust based on cluster size\n",
    "    logger.info(f\"num_partitions {num_partitions}\")\n",
    "    train_path = os.path.join(output_dir, f\"train_{uuid.uuid4().hex}\")\n",
    "    test_path = os.path.join(output_dir, f\"test_{uuid.uuid4().hex}\")\n",
    "    sst2_test_path = os.path.join(output_dir, f\"sst2_{uuid.uuid4().hex}\")\n",
    "    \n",
    "    _start_time = time.time()\n",
    "    logger.info(f\"Writing Parquet files: train={train_path}, test={test_path}, sst2={sst2_test_path}\")\n",
    "    train_df.select(\"input_ids\", \"attention_mask\", \"label\").repartition(num_partitions).write.mode(\"overwrite\").parquet(train_path)\n",
    "    logger.info(f\"{time.time()-_start_time:.4f}s for train_df partition\")\n",
    "    # _start_time = time.time()\n",
    "    # test_df.select(\"input_ids\", \"attention_mask\", \"label\").repartition(num_partitions).write.mode(\"overwrite\").parquet(test_path)\n",
    "    # logger.info(f\"{time.time()-_start_time:.4f}s for test_df partition\")\n",
    "    # _start_time = time.time()\n",
    "    # sst2_test_df.select(\"input_ids\", \"attention_mask\", \"label\").repartition(num_partitions).write.mode(\"overwrite\").parquet(sst2_test_path)\n",
    "    # logger.info(f\"{time.time()-_start_time:.4f}s for sst2_df partition\")\n",
    "    \n",
    "    # Store processed data in MongoDB for reference\n",
    "    # train_collection = f\"train_{uuid.uuid4().hex}\"\n",
    "    # test_collection = f\"test_{uuid.uuid4().hex}\"\n",
    "    # sst2_collection = f\"sst2_{uuid.uuid4().hex}\"\n",
    "    # train_df.write.format(\"mongo\").option(\"collection\", train_collection).mode(\"overwrite\").save()\n",
    "    # test_df.write.format(\"mongo\").option(\"collection\", test_collection).mode(\"overwrite\").save()\n",
    "    # sst2_test_df.write.format(\"mongo\").option(\"collection\", sst2_collection).mode(\"overwrite\").save()\n",
    "    \n",
    "    preprocess_time = time.time() - start_time\n",
    "    return train_path, test_path, sst2_test_path, train_collection, test_collection, sst2_collection, preprocess_time\n",
    "\n",
    "# Check for cached Parquet files\n",
    "def check_cached_parquet(output_dir):\n",
    "    train_path = test_path = sst2_test_path = None\n",
    "    train_collection = test_collection = sst2_collection = None\n",
    "    \n",
    "    for dir_name in os.listdir(output_dir):\n",
    "        if dir_name.startswith(\"train_\"):\n",
    "            train_path = os.path.join(output_dir, dir_name)\n",
    "            train_collection = dir_name\n",
    "        elif dir_name.startswith(\"test_\"):\n",
    "            test_path = os.path.join(output_dir, dir_name)\n",
    "            test_collection = dir_name\n",
    "        elif dir_name.startswith(\"sst2_\"):\n",
    "            sst2_test_path = os.path.join(output_dir, dir_name)\n",
    "            sst2_collection = dir_name\n",
    "    \n",
    "    if train_path and test_path and sst2_test_path:\n",
    "        logger.info(f\"Found cached Parquet files: train={train_path}, test={test_path}, sst2={sst2_test_path}\")\n",
    "        return train_path, test_path, sst2_test_path, train_collection, test_collection, sst2_collection\n",
    "    return None\n",
    "\n",
    "# Lazy-loading Parquet dataset\n",
    "class LazyParquetDataset(IterableDataset):\n",
    "    def __init__(self, parquet_path, rank, world_size, batch_size=1000):\n",
    "        self.parquet_files = sorted(glob.glob(os.path.join(parquet_path, \"*.parquet\")))\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Shard files across ranks\n",
    "        files_per_rank = len(self.parquet_files) // world_size\n",
    "        start_idx = rank * files_per_rank\n",
    "        end_idx = (rank + 1) * files_per_rank if rank < world_size - 1 else len(self.parquet_files)\n",
    "        self.parquet_files = self.parquet_files[start_idx:end_idx]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for file in self.parquet_files:\n",
    "            logger.debug(f\"Rank {self.rank} reading Parquet file: {file}\")\n",
    "            parquet_file = pq.ParquetFile(file)\n",
    "            for batch in parquet_file.iter_batches(batch_size=self.batch_size):\n",
    "                df = batch.to_pandas()\n",
    "                for _, row in df.iterrows():\n",
    "                    yield {\n",
    "                        \"input_ids\": torch.tensor(row[\"input_ids\"], dtype=torch.long),\n",
    "                        \"attention_mask\": torch.tensor(row[\"attention_mask\"], dtype=torch.long),\n",
    "                        \"labels\": torch.tensor(row[\"label\"], dtype=torch.long)\n",
    "                    }\n",
    "\n",
    "# Training and evaluation\n",
    "def train_and_evaluate(rank, world_size, train_path, test_path, sst2_test_path, train_collection, test_collection, sst2_collection, finetune_time, batch_size=8, epochs=3):\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"12345\"\n",
    "    dist.init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "    \n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "    model = DDP(model.to(rank), device_ids=[rank])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = LazyParquetDataset(train_path, rank, world_size)\n",
    "    test_dataset = LazyParquetDataset(test_path, rank, world_size)\n",
    "    sst2_test_dataset = LazyParquetDataset(sst2_test_path, rank, world_size)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=0)\n",
    "    sst2_test_loader = DataLoader(sst2_test_dataset, batch_size=batch_size, num_workers=0)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    # scaler = torch.cuda.amp.GradScaler()  # For mixed-precision training\n",
    "    scaler = torch.amp.GradScaler('cuda')  # For mixed-precision training\n",
    "    \n",
    "    # Measure training wall time\n",
    "    dist.barrier()  # Synchronize all ranks before timing\n",
    "    train_start_time = time.time()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(rank)\n",
    "            attention_mask = batch[\"attention_mask\"].to(rank)\n",
    "            labels = batch[\"labels\"].to(rank)\n",
    "            \n",
    "            # with torch.cuda.amp.autocast():\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        logger.info(f\"GPU[{rank}], Epoch {epoch+1}, Avg Loss: {total_loss / num_batches:.4f}\")\n",
    "    \n",
    "    dist.barrier()  # Synchronize all ranks after training\n",
    "    train_end_time = time.time()\n",
    "    train_wall_time = train_end_time - train_start_time\n",
    "    \n",
    "    # Aggregate max training time across ranks\n",
    "    train_wall_time_tensor = torch.tensor(train_wall_time, dtype=torch.float64).cuda(rank)\n",
    "    dist.all_reduce(train_wall_time_tensor, op=dist.ReduceOp.MAX)\n",
    "    train_wall_time_max = train_wall_time_tensor.item()\n",
    "    \n",
    "    # Log training time only from rank 0\n",
    "    if rank == 0:\n",
    "        finetune_time[0] = train_wall_time_max\n",
    "        logger.info(f\"Training wall time (max across ranks): {train_wall_time_max:.2f} seconds\")\n",
    "    \n",
    "    model.eval()\n",
    "    for dataset_name, loader in [(\"IMDB Test\", test_loader), (\"SST-2 Test\", sst2_test_loader)]:\n",
    "        correct = total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                input_ids = batch[\"input_ids\"].to(rank)\n",
    "                attention_mask = batch[\"attention_mask\"].to(rank)\n",
    "                labels = batch[\"labels\"].to(rank)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "                correct += (predictions == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        logger.info(f\"GPU[{rank}]: {dataset_name} Accuracy: {correct / total:.4f}\")\n",
    "    \n",
    "    dist.destroy_process_group()\n",
    "\n",
    "# Main\n",
    "NUM_CPUs = 4\n",
    "NUM_GPUs = 1\n",
    "logger.info(\"Initializing Spark...\")\n",
    "# os.environ['PYSPARK_PYTHON'] = '/home/goodh/miniconda3/envs/5003/bin/python'\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = '/home/goodh/miniconda3/envs/5003/bin/python'\n",
    "spark = init_spark(NUM_CPUs)\n",
    "\n",
    "# Output directory for Parquet files\n",
    "output_dir = \"processed_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Check for cached Parquet files\n",
    "# cached_data = check_cached_parquet(output_dir)\n",
    "cached_data = None\n",
    "train_path = test_path = sst2_test_path = train_collection = test_collection = sst2_collection = None\n",
    "preprocess_time = 0\n",
    "\n",
    "if cached_data:\n",
    "    logger.info(\"Cached Parquet files found. Skipping data loading and preprocessing...\")\n",
    "    train_path, test_path, sst2_test_path, train_collection, test_collection, sst2_collection = cached_data\n",
    "else:\n",
    "    # Load and preprocess data\n",
    "    logger.info(\"No cached Parquet files found. Running full pipeline...\")\n",
    "    logger.info(\"Loading data to MongoDB...\")\n",
    "    load_data_time, imdb_spark_df, sst2_spark_df = load_data_to_mongodb(spark)\n",
    "    logger.info(f\"Data loading to MongoDB took {load_data_time:.2f} seconds\")\n",
    "    \n",
    "    logger.info(\"Distributed preprocessing and saving to Parquet...\")\n",
    "    train_path, test_path, sst2_test_path, train_collection, test_collection, sst2_collection, preprocess_time = preprocess_data(spark, imdb_spark_df, sst2_spark_df, output_dir)\n",
    "    logger.info(f\"Distributed preprocessing took {preprocess_time:.2f} seconds\")\n",
    "\n",
    "# Run distributed training\n",
    "world_size = NUM_GPUs if NUM_GPUs else max(1, torch.cuda.device_count())\n",
    "logger.info(f\"Using {world_size} GPU(s)\")\n",
    "\n",
    "logger.info(\"Distributed fine-tuning...\")\n",
    "import torch.multiprocessing as mp\n",
    "finetune_time = torch.zeros(world_size, dtype=torch.float32).share_memory_()\n",
    "mp.spawn(\n",
    "    train_and_evaluate,\n",
    "    args=(world_size, train_path, test_path, sst2_test_path, train_collection, test_collection, sst2_collection, finetune_time),\n",
    "    nprocs=world_size,\n",
    "    join=True\n",
    ")\n",
    "\n",
    "# append results\n",
    "result = f\"{time.strftime('%Y/%m/%d-%H:%M:%S')}\\t{NUM_CPUs}\\t\\t{NUM_GPUs}\\t\\t{preprocess_time:.2f}\\t\\t{finetune_time[0]:.2f}\\n\"\n",
    "logger.info(result)\n",
    "with open(\"out/results.out\", \"a\") as f:\n",
    "    f.write(result)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e2280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a525be32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f349eb31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf663cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ea2dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808c105c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15545af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ca6e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b153cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e58b5a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e940a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f7b610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca49379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7fd252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf59fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51de62de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing Spark...\n",
      "INFO:__main__:28 cores for spark\n",
      "25/04/22 22:38:20 WARN Utils: Your hostname, yPC resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/04/22 22:38:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/goodh/.ivy2/cache\n",
      "The jars for the packages stored in: /home/goodh/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-286c017c-2100-4589-b54a-b6d869fae13a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 115ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-286c017c-2100-4589-b54a-b6d869fae13a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/3ms)\n",
      "25/04/22 22:38:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "INFO:__main__:No cached Parquet files found. Running full pipeline...\n",
      "INFO:__main__:Loading data to MongoDB...\n",
      "INFO:__main__:Loading IMDB dataset...\n",
      "INFO:__main__:Writing datasets to MongoDB...\n",
      "25/04/22 22:38:41 WARN TaskSetManager: Stage 0 contains a task of very large size (1327 KiB). The maximum recommended task size is 1000 KiB.\n",
      "INFO:__main__:Data loading to MongoDB took 27.52 seconds                        \n",
      "INFO:__main__:Distributed preprocessing and saving to Parquet...\n",
      "INFO:__main__:Reading data from MongoDB...\n",
      "INFO:__main__:Tokenizing data...\n",
      "INFO:__main__:num_partitions 28\n",
      "INFO:__main__:Writing Parquet files: train=processed_data/train_85bf8976bc45451a801986804669a994, test=None, sst2=None\n",
      "INFO:py4j.clientserver:Error while receiving.                     (13 + 9) / 22]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=82>\n",
      "INFO:py4j.clientserver:Closing down clientserver connection\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=82>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "INFO:py4j.clientserver:Error while receiving.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/socket.py\", line 716, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o34.sc\n",
      "INFO:py4j.clientserver:Closing down clientserver connection\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/socket.py\", line 716, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o34.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "INFO:py4j.clientserver:Closing down clientserver connection\n",
      "25/04/22 22:43:10 ERROR Executor: Exception in task 9.0 in stage 2.0 (TID 38)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_77201/11767734.py\", line 78, in tokenize_batch\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2887, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2975, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3177, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 887, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 854, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 697, in tokenize\n",
      "    tokenized_text.extend(self._tokenize(token))\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 161, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 339, in tokenize\n",
      "    text = self._clean_text(text)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 441, in _clean_text\n",
      "    if cp == 0 or cp == 0xFFFD or _is_control(char):\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/04/22 22:43:10 ERROR Executor: Exception in task 1.0 in stage 2.0 (TID 30)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_77201/11767734.py\", line 78, in tokenize_batch\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2887, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2975, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3177, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 887, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 854, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 697, in tokenize\n",
      "    tokenized_text.extend(self._tokenize(token))\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 161, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 356, in tokenize\n",
      "    token = token.lower()\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/04/22 22:43:10 ERROR Executor: Exception in task 6.0 in stage 2.0 (TID 35)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_77201/11767734.py\", line 78, in tokenize_batch\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2887, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2975, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3177, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 887, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 854, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 653, in tokenize\n",
      "    text = re.sub(pattern, lambda m: m.groups()[0] or m.groups()[1].lower(), text)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/re.py\", line 210, in sub\n",
      "    return _compile(pattern, flags).sub(repl, string, count)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 653, in <lambda>\n",
      "    text = re.sub(pattern, lambda m: m.groups()[0] or m.groups()[1].lower(), text)\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/04/22 22:43:10 ERROR Executor: Exception in task 19.0 in stage 2.0 (TID 48)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_77201/11767734.py\", line 78, in tokenize_batch\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2887, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2975, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3177, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 887, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 854, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 697, in tokenize\n",
      "    tokenized_text.extend(self._tokenize(token))\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 161, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 358, in tokenize\n",
      "    token = self._run_strip_accents(token)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 370, in _run_strip_accents\n",
      "    for char in text:\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/04/22 22:43:10 ERROR Executor: Exception in task 20.0 in stage 2.0 (TID 49)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_77201/11767734.py\", line 78, in tokenize_batch\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2887, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2975, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3177, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 887, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 854, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 697, in tokenize\n",
      "    tokenized_text.extend(self._tokenize(token))\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 161, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 348, in tokenize\n",
      "    text = self._tokenize_chinese_chars(text)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 404, in _tokenize_chinese_chars\n",
      "    if self._is_chinese_char(cp):\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 426, in _is_chinese_char\n",
      "    or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/04/22 22:43:10 ERROR Executor: Exception in task 7.0 in stage 2.0 (TID 36)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_77201/11767734.py\", line 78, in tokenize_batch\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2887, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2975, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3177, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 887, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 854, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 697, in tokenize\n",
      "    tokenized_text.extend(self._tokenize(token))\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 161, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 361, in tokenize\n",
      "    split_tokens.extend(self._run_split_on_punc(token, never_split))\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 387, in _run_split_on_punc\n",
      "    if _is_punctuation(char):\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 375, in _is_punctuation\n",
      "    cat = unicodedata.category(char)\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/04/22 22:43:10 ERROR Executor: Exception in task 17.0 in stage 2.0 (TID 46)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_77201/11767734.py\", line 78, in tokenize_batch\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2887, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2975, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3177, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 887, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 854, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 661, in tokenize\n",
      "    tokens = self.tokens_trie.split(text)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 160, in split\n",
      "    to_remove = set()\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/04/22 22:43:10 WARN TaskSetManager: Lost task 20.0 in stage 2.0 (TID 49) (10.255.255.254 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_77201/11767734.py\", line 78, in tokenize_batch\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2887, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2975, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3177, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 887, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 854, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 697, in tokenize\n",
      "    tokenized_text.extend(self._tokenize(token))\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 161, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 348, in tokenize\n",
      "    text = self._tokenize_chinese_chars(text)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 404, in _tokenize_chinese_chars\n",
      "    if self._is_chinese_char(cp):\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 426, in _is_chinese_char\n",
      "    or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/04/22 22:43:10 ERROR TaskSetManager: Task 20 in stage 2.0 failed 1 times; aborting job\n",
      "25/04/22 22:43:10 WARN TaskSetManager: Lost task 1.0 in stage 2.0 (TID 30) (10.255.255.254 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_77201/11767734.py\", line 78, in tokenize_batch\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2887, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2975, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3177, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 887, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 854, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 697, in tokenize\n",
      "    tokenized_text.extend(self._tokenize(token))\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 161, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 356, in tokenize\n",
      "    token = token.lower()\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/04/22 22:43:10 WARN TaskSetManager: Lost task 17.0 in stage 2.0 (TID 46) (10.255.255.254 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_77201/11767734.py\", line 78, in tokenize_batch\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2887, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2975, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3177, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 887, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 854, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 661, in tokenize\n",
      "    tokens = self.tokens_trie.split(text)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 160, in split\n",
      "    to_remove = set()\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/04/22 22:43:10 WARN TaskSetManager: Lost task 6.0 in stage 2.0 (TID 35) (10.255.255.254 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_77201/11767734.py\", line 78, in tokenize_batch\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2887, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2975, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3177, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 887, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 854, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 653, in tokenize\n",
      "    text = re.sub(pattern, lambda m: m.groups()[0] or m.groups()[1].lower(), text)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/re.py\", line 210, in sub\n",
      "    return _compile(pattern, flags).sub(repl, string, count)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 653, in <lambda>\n",
      "    text = re.sub(pattern, lambda m: m.groups()[0] or m.groups()[1].lower(), text)\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/04/22 22:43:10 WARN TaskSetManager: Lost task 19.0 in stage 2.0 (TID 48) (10.255.255.254 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_77201/11767734.py\", line 78, in tokenize_batch\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2887, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2975, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3177, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 887, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 854, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 697, in tokenize\n",
      "    tokenized_text.extend(self._tokenize(token))\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 161, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 358, in tokenize\n",
      "    token = self._run_strip_accents(token)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 370, in _run_strip_accents\n",
      "    for char in text:\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/04/22 22:43:10 WARN TaskSetManager: Lost task 9.0 in stage 2.0 (TID 38) (10.255.255.254 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_77201/11767734.py\", line 78, in tokenize_batch\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2887, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2975, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3177, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 887, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 854, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 697, in tokenize\n",
      "    tokenized_text.extend(self._tokenize(token))\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 161, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 339, in tokenize\n",
      "    text = self._clean_text(text)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 441, in _clean_text\n",
      "    if cp == 0 or cp == 0xFFFD or _is_control(char):\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/04/22 22:43:10 WARN TaskSetManager: Lost task 7.0 in stage 2.0 (TID 36) (10.255.255.254 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_77201/11767734.py\", line 78, in tokenize_batch\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2887, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2975, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3177, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 887, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 854, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 697, in tokenize\n",
      "    tokenized_text.extend(self._tokenize(token))\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 161, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 361, in tokenize\n",
      "    split_tokens.extend(self._run_split_on_punc(token, never_split))\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 387, in _run_split_on_punc\n",
      "    if _is_punctuation(char):\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 375, in _is_punctuation\n",
      "    cat = unicodedata.category(char)\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/04/22 22:43:10 ERROR FileFormatWriter: Aborting job 90e93ffb-9c0f-4955-bce6-987ee3d4b472.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 2.0 failed 1 times, most recent failure: Lost task 20.0 in stage 2.0 (TID 49) (10.255.255.254 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_77201/11767734.py\", line 78, in tokenize_batch\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2887, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2975, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3177, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 887, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 854, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 697, in tokenize\n",
      "    tokenized_text.extend(self._tokenize(token))\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 161, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 348, in tokenize\n",
      "    text = self._tokenize_chinese_chars(text)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 404, in _tokenize_chinese_chars\n",
      "    if self._is_chinese_char(cp):\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 426, in _is_chinese_char\n",
      "    or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_77201/11767734.py\", line 78, in tokenize_batch\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2887, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2975, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3177, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 887, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 854, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 697, in tokenize\n",
      "    tokenized_text.extend(self._tokenize(token))\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 161, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 348, in tokenize\n",
      "    text = self._tokenize_chinese_chars(text)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 404, in _tokenize_chinese_chars\n",
      "    if self._is_chinese_char(cp):\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 426, in _is_chinese_char\n",
      "    or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "25/04/22 22:43:10 WARN TaskSetManager: Lost task 16.0 in stage 2.0 (TID 45) (10.255.255.254 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 20 in stage 2.0 failed 1 times, most recent failure: Lost task 20.0 in stage 2.0 (TID 49) (10.255.255.254 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_77201/11767734.py\", line 78, in tokenize_batch\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2887, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 2975, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\", line 3177, in batch_encode_plus\n",
      "    return self._batch_encode_plus(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 887, in _batch_encode_plus\n",
      "    first_ids = get_input_ids(ids)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 854, in get_input_ids\n",
      "    tokens = self.tokenize(text, **kwargs)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/tokenization_utils.py\", line 697, in tokenize\n",
      "    tokenized_text.extend(self._tokenize(token))\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 161, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 348, in tokenize\n",
      "    text = self._tokenize_chinese_chars(text)\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 404, in _tokenize_chinese_chars\n",
      "    if self._is_chinese_char(cp):\n",
      "  File \"/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/transformers/models/bert/tokenization_bert.py\", line 426, in _is_chinese_char\n",
      "    or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o131.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 314\u001b[0m\n\u001b[1;32m    311\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData loading to MongoDB took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mload_data_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    313\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistributed preprocessing and saving to Parquet...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 314\u001b[0m     train_path, test_path, sst2_test_path, train_collection, test_collection, sst2_collection, preprocess_time \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistributed preprocessing took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreprocess_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# Run distributed training\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 135\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[0;34m(spark, output_dir, max_length)\u001b[0m\n\u001b[1;32m    133\u001b[0m _start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    134\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWriting Parquet files: train=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, test=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, sst2=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msst2_test_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 135\u001b[0m \u001b[43mtrain_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39m_start_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms for train_df partition\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# _start_time = time.time()\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# test_df.select(\"input_ids\", \"attention_mask\", \"label\").repartition(num_partitions).write.mode(\"overwrite\").parquet(test_path)\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# logger.info(f\"{time.time()-_start_time:.4f}s for test_df partition\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m \n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Store processed data in MongoDB for reference\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/5003/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/5003/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/5003/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/miniconda3/envs/5003/lib/python3.9/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o131.parquet"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, length\n",
    "from pyspark.sql.types import ArrayType, IntegerType, StructType, StructField\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import os\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import pyarrow.parquet as pq\n",
    "import glob\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Spark with MongoDB connector\n",
    "def init_spark(num_cpus = None):\n",
    "    # if num_spark_executor_core: logger.info(f\"{num_spark_executor_core} cores for executor\")\n",
    "    # else: logger.info(f\"number of cores for executor UNDEFINED\")\n",
    "    if num_cpus: logger.info(f\"{num_cpus} cores for spark\")\n",
    "    else: logger.info(f\"num_cpus UNDEFINED\")\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Distributed BERT Fine-Tuning with Preprocessing\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", 28) \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.cores\", 4) \\\n",
    "        .config(\"spark.executor.instances\", 7) \\\n",
    "        .config(\"spark.cores.max\", num_cpus) \\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/sentiment_db.reviews\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/sentiment_db.reviews\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "        .getOrCreate()\n",
    "        # .config(\"spark.mongodb.input.partitionerOptions.partitionSizeMB\", \"256\") \\\n",
    "        # .config(\"spark.driver.cores\", \"2\") \\\n",
    "        # .config(\"spark.default.parallelism\", 10) \\\n",
    "    return spark\n",
    "\n",
    "# Load IMDB and SST-2 data to MongoDB\n",
    "def load_data_to_mongodb(spark):\n",
    "    # IMDB dataset\n",
    "    start_time = time.time()\n",
    "    logger.info(\"Loading IMDB dataset...\")\n",
    "    imdb_dataset = load_dataset(\"imdb\")\n",
    "    imdb_df = pd.concat([\n",
    "        imdb_dataset[\"train\"].to_pandas()[[\"text\", \"label\"]],  # use 50 for debug\n",
    "        imdb_dataset[\"test\"].to_pandas()[[\"text\", \"label\"]]\n",
    "    ])\n",
    "    imdb_df[\"source\"] = \"IMDB\"\n",
    "    imdb_spark_df = spark.createDataFrame(imdb_df).select(col(\"text\"), col(\"label\").cast(\"integer\"), col(\"source\"))\n",
    "    \n",
    "    # # SST-2 dataset\n",
    "    # logger.info(\"Loading SST-2 dataset...\")\n",
    "    # sst2_dataset = load_dataset(\"glue\", \"sst2\")\n",
    "    # sst2_df = sst2_dataset[\"train\"].to_pandas()[[\"sentence\", \"label\"]]  # use 50 for debug\n",
    "    # sst2_df = sst2_df.rename(columns={\"sentence\": \"text\"})\n",
    "    # sst2_df[\"source\"] = \"SST-2\"\n",
    "    # sst2_spark_df = spark.createDataFrame(sst2_df).select(col(\"text\"), col(\"label\").cast(\"integer\"), col(\"source\"))\n",
    "    \n",
    "    logger.info(\"Writing datasets to MongoDB...\")\n",
    "    imdb_spark_df.write.format(\"mongo\").mode(\"append\").save()\n",
    "    # sst2_spark_df.write.format(\"mongo\").mode(\"append\").save()\n",
    "    return time.time() - start_time\n",
    "\n",
    "# Batch tokenizer UDF\n",
    "def create_batch_tokenizer_udf(max_length=128):\n",
    "    def tokenize_batch(texts: pd.Series) -> pd.DataFrame:\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        encodings = tokenizer(\n",
    "            texts.tolist(),\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"np\"\n",
    "        )\n",
    "        return pd.DataFrame({\n",
    "            \"input_ids\": [ids.tolist() for ids in encodings[\"input_ids\"]],\n",
    "            \"attention_mask\": [mask.tolist() for mask in encodings[\"attention_mask\"]]\n",
    "        })\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"input_ids\", ArrayType(IntegerType())),\n",
    "        StructField(\"attention_mask\", ArrayType(IntegerType()))\n",
    "    ])\n",
    "    \n",
    "    return pandas_udf(tokenize_batch, schema)\n",
    "\n",
    "# Preprocess data and save to Parquet\n",
    "def preprocess_data(spark, output_dir, max_length=128):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load and preprocess data\n",
    "    logger.info(\"Reading data from MongoDB...\")\n",
    "    # num_partitions = max(16, spark.sparkContext.defaultParallelism * 2)  # Adjust based on cluster size\n",
    "    num_partitions = 28\n",
    "    raw_df = spark.read.format(\"mongo\").load()\n",
    "    raw_df.repartition(num_partitions)\n",
    "    processed_df = raw_df.filter(length(col(\"text\")) >= 10)\n",
    "    \n",
    "    # Apply distributed batch tokenization\n",
    "    logger.info(\"Tokenizing data...\")\n",
    "    tokenize_udf = create_batch_tokenizer_udf(max_length)\n",
    "    tokenized_df = processed_df.withColumn(\"encoding\", tokenize_udf(col(\"text\")))\n",
    "    \n",
    "    # Extract input_ids and attention_mask\n",
    "    tokenized_df = tokenized_df.select(\n",
    "        col(\"label\").cast(\"integer\").alias(\"label\"),\n",
    "        col(\"source\"),\n",
    "        col(\"encoding.input_ids\").alias(\"input_ids\"),\n",
    "        col(\"encoding.attention_mask\").alias(\"attention_mask\")\n",
    "    )\n",
    "    \n",
    "    # Split IMDB into train/test\n",
    "    imdb_df = tokenized_df.filter(col(\"source\") == \"IMDB\")\n",
    "    train_df, test_df = imdb_df.randomSplit([0.8, 0.2], seed=42)\n",
    "    sst2_test_df = tokenized_df.filter(col(\"source\") == \"SST-2\")\n",
    "    \n",
    "    # Save to Parquet with dynamic partitioning\n",
    "    logger.info(f\"num_partitions {num_partitions}\")\n",
    "    train_path = os.path.join(output_dir, f\"train_{uuid.uuid4().hex}\")\n",
    "    # test_path = os.path.join(output_dir, f\"test_{uuid.uuid4().hex}\")\n",
    "    # sst2_test_path = os.path.join(output_dir, f\"sst2_{uuid.uuid4().hex}\")\n",
    "    \n",
    "    _start_time = time.time()\n",
    "    logger.info(f\"Writing Parquet files: train={train_path}, test={test_path}, sst2={sst2_test_path}\")\n",
    "    train_df.select(\"input_ids\", \"attention_mask\", \"label\").write.mode(\"overwrite\").parquet(train_path)\n",
    "    logger.info(f\"{time.time()-_start_time:.4f}s for train_df partition\")\n",
    "    # _start_time = time.time()\n",
    "    # test_df.select(\"input_ids\", \"attention_mask\", \"label\").repartition(num_partitions).write.mode(\"overwrite\").parquet(test_path)\n",
    "    # logger.info(f\"{time.time()-_start_time:.4f}s for test_df partition\")\n",
    "    # _start_time = time.time()\n",
    "    # sst2_test_df.select(\"input_ids\", \"attention_mask\", \"label\").repartition(num_partitions).write.mode(\"overwrite\").parquet(sst2_test_path)\n",
    "    # logger.info(f\"{time.time()-_start_time:.4f}s for sst2_df partition\")\n",
    "    \n",
    "    # Store processed data in MongoDB for reference\n",
    "    train_collection = f\"train_{uuid.uuid4().hex}\"\n",
    "    # test_collection = f\"test_{uuid.uuid4().hex}\"\n",
    "    # sst2_collection = f\"sst2_{uuid.uuid4().hex}\"\n",
    "    train_df.write.format(\"mongo\").option(\"collection\", train_collection).mode(\"overwrite\").save()\n",
    "    # test_df.write.format(\"mongo\").option(\"collection\", test_collection).mode(\"overwrite\").save()\n",
    "    # sst2_test_df.write.format(\"mongo\").option(\"collection\", sst2_collection).mode(\"overwrite\").save()\n",
    "    \n",
    "    preprocess_time = time.time() - start_time\n",
    "    return train_path, test_path, sst2_test_path, train_collection, test_collection, sst2_collection, preprocess_time\n",
    "\n",
    "# Check for cached Parquet files\n",
    "def check_cached_parquet(output_dir):\n",
    "    train_path = test_path = sst2_test_path = None\n",
    "    train_collection = test_collection = sst2_collection = None\n",
    "    \n",
    "    for dir_name in os.listdir(output_dir):\n",
    "        if dir_name.startswith(\"train_\"):\n",
    "            train_path = os.path.join(output_dir, dir_name)\n",
    "            train_collection = dir_name\n",
    "        elif dir_name.startswith(\"test_\"):\n",
    "            test_path = os.path.join(output_dir, dir_name)\n",
    "            test_collection = dir_name\n",
    "        elif dir_name.startswith(\"sst2_\"):\n",
    "            sst2_test_path = os.path.join(output_dir, dir_name)\n",
    "            sst2_collection = dir_name\n",
    "    \n",
    "    if train_path and test_path and sst2_test_path:\n",
    "        logger.info(f\"Found cached Parquet files: train={train_path}, test={test_path}, sst2={sst2_test_path}\")\n",
    "        return train_path, test_path, sst2_test_path, train_collection, test_collection, sst2_collection\n",
    "    return None\n",
    "\n",
    "# Lazy-loading Parquet dataset\n",
    "class LazyParquetDataset(IterableDataset):\n",
    "    def __init__(self, parquet_path, rank, world_size, batch_size=1000):\n",
    "        self.parquet_files = sorted(glob.glob(os.path.join(parquet_path, \"*.parquet\")))\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Shard files across ranks\n",
    "        files_per_rank = len(self.parquet_files) // world_size\n",
    "        start_idx = rank * files_per_rank\n",
    "        end_idx = (rank + 1) * files_per_rank if rank < world_size - 1 else len(self.parquet_files)\n",
    "        self.parquet_files = self.parquet_files[start_idx:end_idx]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for file in self.parquet_files:\n",
    "            logger.debug(f\"Rank {self.rank} reading Parquet file: {file}\")\n",
    "            parquet_file = pq.ParquetFile(file)\n",
    "            for batch in parquet_file.iter_batches(batch_size=self.batch_size):\n",
    "                df = batch.to_pandas()\n",
    "                for _, row in df.iterrows():\n",
    "                    yield {\n",
    "                        \"input_ids\": torch.tensor(row[\"input_ids\"], dtype=torch.long),\n",
    "                        \"attention_mask\": torch.tensor(row[\"attention_mask\"], dtype=torch.long),\n",
    "                        \"labels\": torch.tensor(row[\"label\"], dtype=torch.long)\n",
    "                    }\n",
    "\n",
    "# Training and evaluation\n",
    "def train_and_evaluate(rank, world_size, train_path, test_path, sst2_test_path, train_collection, test_collection, sst2_collection, finetune_time, batch_size=8, epochs=3):\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"12345\"\n",
    "    dist.init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "    \n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "    model = DDP(model.to(rank), device_ids=[rank])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = LazyParquetDataset(train_path, rank, world_size)\n",
    "    test_dataset = LazyParquetDataset(test_path, rank, world_size)\n",
    "    sst2_test_dataset = LazyParquetDataset(sst2_test_path, rank, world_size)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=0)\n",
    "    sst2_test_loader = DataLoader(sst2_test_dataset, batch_size=batch_size, num_workers=0)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    # scaler = torch.cuda.amp.GradScaler()  # For mixed-precision training\n",
    "    scaler = torch.amp.GradScaler('cuda')  # For mixed-precision training\n",
    "    \n",
    "    # Measure training wall time\n",
    "    dist.barrier()  # Synchronize all ranks before timing\n",
    "    train_start_time = time.time()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(rank)\n",
    "            attention_mask = batch[\"attention_mask\"].to(rank)\n",
    "            labels = batch[\"labels\"].to(rank)\n",
    "            \n",
    "            # with torch.cuda.amp.autocast():\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        logger.info(f\"GPU[{rank}], Epoch {epoch+1}, Avg Loss: {total_loss / num_batches:.4f}\")\n",
    "    \n",
    "    dist.barrier()  # Synchronize all ranks after training\n",
    "    train_end_time = time.time()\n",
    "    train_wall_time = train_end_time - train_start_time\n",
    "    \n",
    "    # Aggregate max training time across ranks\n",
    "    train_wall_time_tensor = torch.tensor(train_wall_time, dtype=torch.float64).cuda(rank)\n",
    "    dist.all_reduce(train_wall_time_tensor, op=dist.ReduceOp.MAX)\n",
    "    train_wall_time_max = train_wall_time_tensor.item()\n",
    "    \n",
    "    # Log training time only from rank 0\n",
    "    if rank == 0:\n",
    "        finetune_time[0] = train_wall_time_max\n",
    "        logger.info(f\"Training wall time (max across ranks): {train_wall_time_max:.2f} seconds\")\n",
    "    \n",
    "    model.eval()\n",
    "    for dataset_name, loader in [(\"IMDB Test\", test_loader), (\"SST-2 Test\", sst2_test_loader)]:\n",
    "        correct = total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                input_ids = batch[\"input_ids\"].to(rank)\n",
    "                attention_mask = batch[\"attention_mask\"].to(rank)\n",
    "                labels = batch[\"labels\"].to(rank)\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "                correct += (predictions == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        logger.info(f\"GPU[{rank}]: {dataset_name} Accuracy: {correct / total:.4f}\")\n",
    "    \n",
    "    dist.destroy_process_group()\n",
    "\n",
    "# Main\n",
    "# if __name__ == \"__main__\":\n",
    "NUM_CPUs = 28\n",
    "NUM_GPUs = 1\n",
    "logger.info(\"Initializing Spark...\")\n",
    "# os.environ['PYSPARK_PYTHON'] = '/home/goodh/miniconda3/envs/5003/bin/python'\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = '/home/goodh/miniconda3/envs/5003/bin/python'\n",
    "spark = init_spark(NUM_CPUs)\n",
    "\n",
    "# Output directory for Parquet files\n",
    "output_dir = \"processed_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Check for cached Parquet files\n",
    "# cached_data = check_cached_parquet(output_dir)\n",
    "cached_data = None\n",
    "train_path = test_path = sst2_test_path = train_collection = test_collection = sst2_collection = None\n",
    "preprocess_time = 0\n",
    "\n",
    "if cached_data:\n",
    "    logger.info(\"Cached Parquet files found. Skipping data loading and preprocessing...\")\n",
    "    train_path, test_path, sst2_test_path, train_collection, test_collection, sst2_collection = cached_data\n",
    "else:\n",
    "    # Load and preprocess data\n",
    "    logger.info(\"No cached Parquet files found. Running full pipeline...\")\n",
    "    logger.info(\"Loading data to MongoDB...\")\n",
    "    load_data_time = load_data_to_mongodb(spark)\n",
    "    logger.info(f\"Data loading to MongoDB took {load_data_time:.2f} seconds\")\n",
    "    \n",
    "    logger.info(\"Distributed preprocessing and saving to Parquet...\")\n",
    "    train_path, test_path, sst2_test_path, train_collection, test_collection, sst2_collection, preprocess_time = preprocess_data(spark, output_dir)\n",
    "    logger.info(f\"Distributed preprocessing took {preprocess_time:.2f} seconds\")\n",
    "\n",
    "# Run distributed training\n",
    "world_size = NUM_GPUs if NUM_GPUs else max(1, torch.cuda.device_count())\n",
    "logger.info(f\"Using {world_size} GPU(s)\")\n",
    "\n",
    "logger.info(\"Distributed fine-tuning...\")\n",
    "import torch.multiprocessing as mp\n",
    "finetune_time = torch.zeros(world_size, dtype=torch.float32).share_memory_()\n",
    "mp.spawn(\n",
    "    train_and_evaluate,\n",
    "    args=(world_size, train_path, test_path, sst2_test_path, train_collection, test_collection, sst2_collection, finetune_time),\n",
    "    nprocs=world_size,\n",
    "    join=True\n",
    ")\n",
    "\n",
    "# append results\n",
    "result = f\"{time.strftime('%Y/%m/%d-%H:%M:%S')}\\t{NUM_CPUs}\\t\\t{NUM_GPUs}\\t\\t{preprocess_time:.2f}\\t\\t{finetune_time[0]:.2f}\\n\"\n",
    "logger.info(result)\n",
    "with open(\"out/results.out\", \"a\") as f:\n",
    "    f.write(result)\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5003",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
