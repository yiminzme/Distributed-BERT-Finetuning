{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb40d541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo mongod --dbpath ~/data/mongodb\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49582617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, length\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e796ed78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "def init_spark():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Distributed BERT Fine-Tuning\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "# Load and preprocess data (simplified for example)\n",
    "def load_and_preprocess_data(spark):\n",
    "    # Load IMDB dataset (using a small subset for demo)\n",
    "    imdb_dataset = load_dataset(\"imdb\")\n",
    "    imdb_df = imdb_dataset[\"train\"].to_pandas()[[\"text\", \"label\"]].head(100)  # Limit for testing\n",
    "    spark_df = spark.createDataFrame(imdb_df)\n",
    "    \n",
    "    # Filter short texts and standardize labels\n",
    "    processed_df = spark_df.filter(length(col(\"text\")) >= 10)\n",
    "    processed_df = processed_df.withColumn(\"label\", col(\"label\").cast(\"integer\"))\n",
    "    \n",
    "    # Split into train/test\n",
    "    train_df, test_df = processed_df.randomSplit([0.8, 0.2], seed=42)\n",
    "    return train_df, test_df\n",
    "\n",
    "# Custom PyTorch Dataset for Spark DataFrame\n",
    "class SparkDataset(Dataset):\n",
    "    def __init__(self, spark_df, tokenizer, max_length=128):\n",
    "        # Convert Spark DataFrame to Pandas for simplicity (in practice, use Spark's iterator for large data)\n",
    "        self.data = spark_df.toPandas()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx][\"text\"]\n",
    "        label = self.data.iloc[idx][\"label\"]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Initialize BERT model and tokenizer\n",
    "def init_bert():\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "    return tokenizer, model\n",
    "\n",
    "# Setup distributed training\n",
    "def setup_distributed_training(rank, world_size):\n",
    "    if world_size > 1:\n",
    "        os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "        os.environ[\"MASTER_PORT\"] = \"12345\"\n",
    "        dist.init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "# Training function\n",
    "def train_bert(model, train_loader, rank, world_size, epochs=3, batch_size=8):\n",
    "    device = torch.device(f\"cuda:{rank}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Wrap model in DDP if multiple GPUs, else use single GPU\n",
    "    if world_size > 1:\n",
    "        model = DDP(model, device_ids=[rank])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if rank == 0:\n",
    "            print(f\"Epoch {epoch+1}, Avg Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_bert(model, test_loader, rank, dataset_name):\n",
    "    device = torch.device(f\"cuda:{rank}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    if rank == 0:\n",
    "        print(f\"{dataset_name} Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "# Main distributed training function\n",
    "def run_distributed_training(rank, world_size, model, train_dataset, test_dataset, batch_size=8):\n",
    "    # Setup distributed training\n",
    "    setup_distributed_training(rank, world_size)\n",
    "    \n",
    "    # Create distributed DataLoader\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank) if world_size > 1 else None\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=train_sampler,\n",
    "        shuffle=(train_sampler is None),\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Train and evaluate\n",
    "    train_bert(model, train_loader, rank, world_size)\n",
    "    evaluate_bert(model, test_loader, rank, \"Test\")\n",
    "    \n",
    "    # Clean up\n",
    "    if world_size > 1:\n",
    "        dist.destroy_process_group()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0ed8d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/13 20:37:33 WARN Utils: Your hostname, yPC resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/04/13 20:37:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/13 20:37:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 GPU(s)\n",
      "Epoch 1, Avg Loss: 0.4646\n",
      "Epoch 2, Avg Loss: 0.3847\n",
      "Epoch 3, Avg Loss: 0.2759\n",
      "Test Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Spark and load data\n",
    "spark = init_spark()\n",
    "train_df, test_df = load_and_preprocess_data(spark)\n",
    "train_df, test_df = train_df.limit(6), test_df.limit(6)\n",
    "\n",
    "# Initialize BERT\n",
    "tokenizer, model = init_bert()\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = SparkDataset(train_df, tokenizer)\n",
    "test_dataset = SparkDataset(test_df, tokenizer)\n",
    "\n",
    "# GPU setup\n",
    "world_size = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "print(f\"Using {world_size} GPU(s)\")\n",
    "\n",
    "if world_size > 1:\n",
    "    # Multi-GPU: Use torch.multiprocessing\n",
    "    import torch.multiprocessing as mp\n",
    "    mp.spawn(\n",
    "        run_distributed_training,\n",
    "        args=(world_size, model, train_dataset, test_dataset),\n",
    "        nprocs=world_size,\n",
    "        join=True\n",
    "    )\n",
    "else:\n",
    "    # Single GPU or CPU\n",
    "    run_distributed_training(0, 1, model, train_dataset, test_dataset)\n",
    "\n",
    "# Stop Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f9e93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b1c71c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f57247",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5003",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
