srun: error: dgx-43: task 0: Exited with exit code 1
mkdir: cannot create directory ‘mongodb’: File exists
about to fork child process, waiting until server is ready for connections.
forked process: 3785543
child process started successfully, parent exiting
Current time: 20250426_181831, num_cpus[16], num_gpus[1], num_train_samples[2500], batch_size[16], epoches[10]
INFO:__main__:Initializing Spark...
INFO:__main__:16 cores for spark
:: loading settings :: url = jar:file:/home/yzhengbs/anaconda3/envs/5003/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /home/yzhengbs/.ivy2/cache
The jars for the packages stored in: /home/yzhengbs/.ivy2/jars
org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-fb791784-928b-4723-8bf3-b8daef528f23;1.0
	confs: [default]
	found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
	found org.mongodb#mongodb-driver-sync;4.0.5 in central
	found org.mongodb#bson;4.0.5 in central
	found org.mongodb#mongodb-driver-core;4.0.5 in central
:: resolution report :: resolve 150ms :: artifacts dl 10ms
	:: modules in use:
	org.mongodb#bson;4.0.5 from central in [default]
	org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
	org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
	org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-fb791784-928b-4723-8bf3-b8daef528f23
	confs: [default]
	0 artifacts copied, 4 already retrieved (0kB/5ms)
25/04/26 10:18:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
INFO:__main__:No cached Parquet files found. Running full pipeline...
INFO:__main__:Loading data to MongoDB...
INFO:__main__:Loading IMDB dataset...
INFO:__main__:Loading SST-2 dataset...
INFO:__main__:Writing datasets to MongoDB...
25/04/26 10:19:02 WARN TaskSetManager: Stage 0 contains a task of very large size (3984 KiB). The maximum recommended task size is 1000 KiB.

[Stage 0:>                                                        (0 + 16) / 16]

[Stage 0:===>                                                     (1 + 15) / 16]

[Stage 0:===================================>                     (10 + 6) / 16]

                                                                                
INFO:__main__:Data loading to MongoDB took 24.06 seconds
INFO:__main__:Distributed preprocessing and saving to Parquet...
INFO:__main__:Reading data from MongoDB...
INFO:__main__:Applying text preprocessing (lowercase, remove punctuation, non-alnum, continuous whitespace, strip)...
INFO:__main__:Tokenizing data...
INFO:__main__:num_samples[2500]
INFO:__main__:num_partitions 16
INFO:__main__:Writing Parquet files: train=processed_data/train_82bcf5fbaf38483ea3117c35bee8aee6, test=processed_data/test_7f56246e975b4b6380a40464da13252f, sst2=processed_data/sst2_bd3580419cfa44bf9c5321956e58fdb4
25/04/26 10:19:05 WARN TaskSetManager: Stage 2 contains a task of very large size (3984 KiB). The maximum recommended task size is 1000 KiB.

[Stage 2:>                                                        (0 + 16) / 32]

[Stage 2:==========>                                              (6 + 16) / 32]

[Stage 2:==============>                                          (8 + 16) / 32]

[Stage 2:================>                                        (9 + 16) / 32]

[Stage 2:======================>                                 (13 + 16) / 32]

[Stage 2:========================>                               (14 + 16) / 32]

[Stage 2:==========================>                             (15 + 16) / 32]

[Stage 2:============================>                           (16 + 16) / 32]

[Stage 2:=============================>                          (17 + 15) / 32]

[Stage 2:===============================>                        (18 + 14) / 32]

[Stage 2:========================================>                (23 + 9) / 32]

[Stage 2:============================================>            (25 + 7) / 32]

[Stage 2:================================================>        (27 + 5) / 32]

[Stage 2:=================================================>       (28 + 4) / 32]

[Stage 2:===================================================>     (29 + 3) / 32]

[Stage 2:=====================================================>   (30 + 2) / 32]

[Stage 2:=======================================================> (31 + 1) / 32]

[Stage 7:>                                                        (0 + 16) / 16]

                                                                                
INFO:__main__:27.9206s for train_df partition
25/04/26 10:19:33 WARN TaskSetManager: Stage 8 contains a task of very large size (3984 KiB). The maximum recommended task size is 1000 KiB.

[Stage 8:>                                                        (0 + 16) / 32]

[Stage 8:=>                                                       (1 + 16) / 32]

[Stage 8:===>                                                     (2 + 16) / 32]

[Stage 8:============>                                            (7 + 16) / 32]

[Stage 8:=================>                                      (10 + 16) / 32]

[Stage 8:============================>                           (16 + 16) / 32]

[Stage 8:=============================>                          (17 + 15) / 32]

[Stage 8:=================================>                      (19 + 13) / 32]

[Stage 8:===================================>                    (20 + 12) / 32]

[Stage 8:====================================>                   (21 + 11) / 32]

[Stage 8:======================================>                 (22 + 10) / 32]

[Stage 8:========================================>                (23 + 9) / 32]

[Stage 8:==============================================>          (26 + 6) / 32]

[Stage 8:===================================================>     (29 + 3) / 32]

[Stage 8:=====================================================>   (30 + 2) / 32]

[Stage 8:=======================================================> (31 + 1) / 32]

                                                                                
INFO:__main__:17.4992s for test_df partition
25/04/26 10:19:50 WARN TaskSetManager: Stage 14 contains a task of very large size (3984 KiB). The maximum recommended task size is 1000 KiB.

[Stage 14:===========================>                           (16 + 16) / 32]

[Stage 14:=============================>                         (17 + 15) / 32]

[Stage 14:=================================================>      (28 + 4) / 32]

[Stage 14:==================================================>     (29 + 3) / 32]

[Stage 14:====================================================>   (30 + 2) / 32]

[Stage 14:======================================================> (31 + 1) / 32]

                                                                                
INFO:__main__:7.4985s for sst2_df partition
INFO:__main__:Distributed preprocessing took 53.07 seconds
INFO:__main__:Using 1 GPU(s)
INFO:__main__:Distributed fine-tuning...
INFO:__mp_main__:Rank 0: Assigned 16 files: ['processed_data/train_82bcf5fbaf38483ea3117c35bee8aee6/part-00000-ada92149-d4a4-4188-9dfa-f622fa799132-c000.snappy.parquet', 'processed_data/train_82bcf5fbaf38483ea3117c35bee8aee6/part-00001-ada92149-d4a4-4188-9dfa-f622fa799132-c000.snappy.parquet', 'processed_data/train_82bcf5fbaf38483ea3117c35bee8aee6/part-00002-ada92149-d4a4-4188-9dfa-f622fa799132-c000.snappy.parquet', 'processed_data/train_82bcf5fbaf38483ea3117c35bee8aee6/part-00003-ada92149-d4a4-4188-9dfa-f622fa799132-c000.snappy.parquet', 'processed_data/train_82bcf5fbaf38483ea3117c35bee8aee6/part-00004-ada92149-d4a4-4188-9dfa-f622fa799132-c000.snappy.parquet', 'processed_data/train_82bcf5fbaf38483ea3117c35bee8aee6/part-00005-ada92149-d4a4-4188-9dfa-f622fa799132-c000.snappy.parquet', 'processed_data/train_82bcf5fbaf38483ea3117c35bee8aee6/part-00006-ada92149-d4a4-4188-9dfa-f622fa799132-c000.snappy.parquet', 'processed_data/train_82bcf5fbaf38483ea3117c35bee8aee6/part-00007-ada92149-d4a4-4188-9dfa-f622fa799132-c000.snappy.parquet', 'processed_data/train_82bcf5fbaf38483ea3117c35bee8aee6/part-00008-ada92149-d4a4-4188-9dfa-f622fa799132-c000.snappy.parquet', 'processed_data/train_82bcf5fbaf38483ea3117c35bee8aee6/part-00009-ada92149-d4a4-4188-9dfa-f622fa799132-c000.snappy.parquet', 'processed_data/train_82bcf5fbaf38483ea3117c35bee8aee6/part-00010-ada92149-d4a4-4188-9dfa-f622fa799132-c000.snappy.parquet', 'processed_data/train_82bcf5fbaf38483ea3117c35bee8aee6/part-00011-ada92149-d4a4-4188-9dfa-f622fa799132-c000.snappy.parquet', 'processed_data/train_82bcf5fbaf38483ea3117c35bee8aee6/part-00012-ada92149-d4a4-4188-9dfa-f622fa799132-c000.snappy.parquet', 'processed_data/train_82bcf5fbaf38483ea3117c35bee8aee6/part-00013-ada92149-d4a4-4188-9dfa-f622fa799132-c000.snappy.parquet', 'processed_data/train_82bcf5fbaf38483ea3117c35bee8aee6/part-00014-ada92149-d4a4-4188-9dfa-f622fa799132-c000.snappy.parquet', 'processed_data/train_82bcf5fbaf38483ea3117c35bee8aee6/part-00015-ada92149-d4a4-4188-9dfa-f622fa799132-c000.snappy.parquet']
INFO:__mp_main__:Rank 0: Assigned 16 files: ['processed_data/test_7f56246e975b4b6380a40464da13252f/part-00000-de5284e6-aae5-404f-a870-b3846a4d25c1-c000.snappy.parquet', 'processed_data/test_7f56246e975b4b6380a40464da13252f/part-00001-de5284e6-aae5-404f-a870-b3846a4d25c1-c000.snappy.parquet', 'processed_data/test_7f56246e975b4b6380a40464da13252f/part-00002-de5284e6-aae5-404f-a870-b3846a4d25c1-c000.snappy.parquet', 'processed_data/test_7f56246e975b4b6380a40464da13252f/part-00003-de5284e6-aae5-404f-a870-b3846a4d25c1-c000.snappy.parquet', 'processed_data/test_7f56246e975b4b6380a40464da13252f/part-00004-de5284e6-aae5-404f-a870-b3846a4d25c1-c000.snappy.parquet', 'processed_data/test_7f56246e975b4b6380a40464da13252f/part-00005-de5284e6-aae5-404f-a870-b3846a4d25c1-c000.snappy.parquet', 'processed_data/test_7f56246e975b4b6380a40464da13252f/part-00006-de5284e6-aae5-404f-a870-b3846a4d25c1-c000.snappy.parquet', 'processed_data/test_7f56246e975b4b6380a40464da13252f/part-00007-de5284e6-aae5-404f-a870-b3846a4d25c1-c000.snappy.parquet', 'processed_data/test_7f56246e975b4b6380a40464da13252f/part-00008-de5284e6-aae5-404f-a870-b3846a4d25c1-c000.snappy.parquet', 'processed_data/test_7f56246e975b4b6380a40464da13252f/part-00009-de5284e6-aae5-404f-a870-b3846a4d25c1-c000.snappy.parquet', 'processed_data/test_7f56246e975b4b6380a40464da13252f/part-00010-de5284e6-aae5-404f-a870-b3846a4d25c1-c000.snappy.parquet', 'processed_data/test_7f56246e975b4b6380a40464da13252f/part-00011-de5284e6-aae5-404f-a870-b3846a4d25c1-c000.snappy.parquet', 'processed_data/test_7f56246e975b4b6380a40464da13252f/part-00012-de5284e6-aae5-404f-a870-b3846a4d25c1-c000.snappy.parquet', 'processed_data/test_7f56246e975b4b6380a40464da13252f/part-00013-de5284e6-aae5-404f-a870-b3846a4d25c1-c000.snappy.parquet', 'processed_data/test_7f56246e975b4b6380a40464da13252f/part-00014-de5284e6-aae5-404f-a870-b3846a4d25c1-c000.snappy.parquet', 'processed_data/test_7f56246e975b4b6380a40464da13252f/part-00015-de5284e6-aae5-404f-a870-b3846a4d25c1-c000.snappy.parquet']
INFO:__mp_main__:Rank 0: Assigned 16 files: ['processed_data/sst2_bd3580419cfa44bf9c5321956e58fdb4/part-00000-07cd4f9e-e4fb-43e7-9404-3cd8b0954849-c000.snappy.parquet', 'processed_data/sst2_bd3580419cfa44bf9c5321956e58fdb4/part-00001-07cd4f9e-e4fb-43e7-9404-3cd8b0954849-c000.snappy.parquet', 'processed_data/sst2_bd3580419cfa44bf9c5321956e58fdb4/part-00002-07cd4f9e-e4fb-43e7-9404-3cd8b0954849-c000.snappy.parquet', 'processed_data/sst2_bd3580419cfa44bf9c5321956e58fdb4/part-00003-07cd4f9e-e4fb-43e7-9404-3cd8b0954849-c000.snappy.parquet', 'processed_data/sst2_bd3580419cfa44bf9c5321956e58fdb4/part-00004-07cd4f9e-e4fb-43e7-9404-3cd8b0954849-c000.snappy.parquet', 'processed_data/sst2_bd3580419cfa44bf9c5321956e58fdb4/part-00005-07cd4f9e-e4fb-43e7-9404-3cd8b0954849-c000.snappy.parquet', 'processed_data/sst2_bd3580419cfa44bf9c5321956e58fdb4/part-00006-07cd4f9e-e4fb-43e7-9404-3cd8b0954849-c000.snappy.parquet', 'processed_data/sst2_bd3580419cfa44bf9c5321956e58fdb4/part-00007-07cd4f9e-e4fb-43e7-9404-3cd8b0954849-c000.snappy.parquet', 'processed_data/sst2_bd3580419cfa44bf9c5321956e58fdb4/part-00008-07cd4f9e-e4fb-43e7-9404-3cd8b0954849-c000.snappy.parquet', 'processed_data/sst2_bd3580419cfa44bf9c5321956e58fdb4/part-00009-07cd4f9e-e4fb-43e7-9404-3cd8b0954849-c000.snappy.parquet', 'processed_data/sst2_bd3580419cfa44bf9c5321956e58fdb4/part-00010-07cd4f9e-e4fb-43e7-9404-3cd8b0954849-c000.snappy.parquet', 'processed_data/sst2_bd3580419cfa44bf9c5321956e58fdb4/part-00011-07cd4f9e-e4fb-43e7-9404-3cd8b0954849-c000.snappy.parquet', 'processed_data/sst2_bd3580419cfa44bf9c5321956e58fdb4/part-00012-07cd4f9e-e4fb-43e7-9404-3cd8b0954849-c000.snappy.parquet', 'processed_data/sst2_bd3580419cfa44bf9c5321956e58fdb4/part-00013-07cd4f9e-e4fb-43e7-9404-3cd8b0954849-c000.snappy.parquet', 'processed_data/sst2_bd3580419cfa44bf9c5321956e58fdb4/part-00014-07cd4f9e-e4fb-43e7-9404-3cd8b0954849-c000.snappy.parquet', 'processed_data/sst2_bd3580419cfa44bf9c5321956e58fdb4/part-00015-07cd4f9e-e4fb-43e7-9404-3cd8b0954849-c000.snappy.parquet']
INFO:__mp_main__:Rank 0: Local train batch count = 156, Global min train batch count = 156
INFO:__mp_main__:Rank 0: Local test batch count = 31, Global min test batch count = 31
INFO:__mp_main__:Rank 0: Local sst2_test batch count = 31, Global min sst2_test batch count = 31
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/yzhengbs/anaconda3/envs/5003/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:444: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.
  warnings.warn(
INFO:__mp_main__:Epoch 1, Avg Training Loss: 0.6970
INFO:__mp_main__:GPU[0], Epoch 1, Avg Loss: 0.6970
INFO:__mp_main__:Epoch 1, IMDB Test Eval Loss: 0.6623, Accuracy: 0.6633
INFO:__mp_main__:Epoch 1, SST-2 Test Eval Loss: 0.6996, Accuracy: 0.4698
INFO:__mp_main__:Epoch 2, Avg Training Loss: 0.6325
INFO:__mp_main__:GPU[0], Epoch 2, Avg Loss: 0.6325
INFO:__mp_main__:Epoch 2, IMDB Test Eval Loss: 0.5412, Accuracy: 0.7440
INFO:__mp_main__:Epoch 2, SST-2 Test Eval Loss: 0.6259, Accuracy: 0.6391
INFO:__mp_main__:Epoch 3, Avg Training Loss: 0.5232
INFO:__mp_main__:GPU[0], Epoch 3, Avg Loss: 0.5232
INFO:__mp_main__:Epoch 3, IMDB Test Eval Loss: 0.5113, Accuracy: 0.7520
INFO:__mp_main__:Epoch 3, SST-2 Test Eval Loss: 0.5607, Accuracy: 0.6915
INFO:__mp_main__:Epoch 4, Avg Training Loss: 0.4544
INFO:__mp_main__:GPU[0], Epoch 4, Avg Loss: 0.4544
INFO:__mp_main__:Epoch 4, IMDB Test Eval Loss: 0.4014, Accuracy: 0.8327
INFO:__mp_main__:Epoch 4, SST-2 Test Eval Loss: 0.5392, Accuracy: 0.7016
INFO:__mp_main__:Epoch 5, Avg Training Loss: 0.4138
INFO:__mp_main__:GPU[0], Epoch 5, Avg Loss: 0.4138
INFO:__mp_main__:Epoch 5, IMDB Test Eval Loss: 0.3661, Accuracy: 0.8528
INFO:__mp_main__:Epoch 5, SST-2 Test Eval Loss: 0.5193, Accuracy: 0.7117
INFO:__mp_main__:Epoch 6, Avg Training Loss: 0.4006
INFO:__mp_main__:GPU[0], Epoch 6, Avg Loss: 0.4006
INFO:__mp_main__:Epoch 6, IMDB Test Eval Loss: 0.3583, Accuracy: 0.8730
INFO:__mp_main__:Epoch 6, SST-2 Test Eval Loss: 0.5026, Accuracy: 0.7258
INFO:__mp_main__:Epoch 7, Avg Training Loss: 0.3872
INFO:__mp_main__:GPU[0], Epoch 7, Avg Loss: 0.3872
INFO:__mp_main__:Epoch 7, IMDB Test Eval Loss: 0.3441, Accuracy: 0.8690
INFO:__mp_main__:Epoch 7, SST-2 Test Eval Loss: 0.5035, Accuracy: 0.7278
INFO:__mp_main__:Epoch 8, Avg Training Loss: 0.3697
INFO:__mp_main__:GPU[0], Epoch 8, Avg Loss: 0.3697
INFO:__mp_main__:Epoch 8, IMDB Test Eval Loss: 0.3357, Accuracy: 0.8690
INFO:__mp_main__:Epoch 8, SST-2 Test Eval Loss: 0.4941, Accuracy: 0.7359
INFO:__mp_main__:Epoch 9, Avg Training Loss: 0.3682
INFO:__mp_main__:GPU[0], Epoch 9, Avg Loss: 0.3682
INFO:__mp_main__:Epoch 9, IMDB Test Eval Loss: 0.3141, Accuracy: 0.8891
INFO:__mp_main__:Epoch 9, SST-2 Test Eval Loss: 0.4827, Accuracy: 0.7480
INFO:__mp_main__:Epoch 10, Avg Training Loss: 0.3753
INFO:__mp_main__:GPU[0], Epoch 10, Avg Loss: 0.3753
INFO:__mp_main__:Epoch 10, IMDB Test Eval Loss: 0.3078, Accuracy: 0.8931
INFO:__mp_main__:Epoch 10, SST-2 Test Eval Loss: 0.4821, Accuracy: 0.7480
INFO:__mp_main__:Training wall time (max across ranks): 56.17 seconds
/home/yzhengbs/anaconda3/envs/5003/lib/python3.9/site-packages/torch/distributed/fsdp/_state_dict_utils.py:768: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.
  warnings.warn(
/home/yzhengbs/anaconda3/envs/5003/lib/python3.9/site-packages/torch/distributed/fsdp/_state_dict_utils.py:711: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.
  warnings.warn(
INFO:__mp_main__:Checkpoint saved at checkpoints/20250426_182102_bert_finetuned_epoch_10.pt
INFO:__mp_main__:Rank 0: IMDB Test local batch count = 31, global min = 31
INFO:__mp_main__:IMDB Test Accuracy: 0.8931451612903226
INFO:__mp_main__:GPU[0]: IMDB Test Accuracy: 0.8931
INFO:__mp_main__:Rank 0: SST-2 Test local batch count = 31, global min = 31
INFO:__mp_main__:SST-2 Test Accuracy: 0.7459677419354839
INFO:__mp_main__:GPU[0]: SST-2 Test Accuracy: 0.7460
/home/yzhengbs/vinc/5003/project/train.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(checkpoint_path)
INFO:__main__:Loss curves saved at plots/loss_curves_20250426_182107.png
INFO:__main__:2025/04/26-18:21:07	NUM_CPUs[16]		NUM_GPUs[1]		NUM_TRAIN_SAMPLES[2500]		preprocess_time[53.07 sec]		finetune_time[56.17 sec]

INFO:py4j.clientserver:Closing down clientserver connection
