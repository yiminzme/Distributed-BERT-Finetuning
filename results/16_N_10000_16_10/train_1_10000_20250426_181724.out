mkdir: cannot create directory ‘mongodb’: File exists
about to fork child process, waiting until server is ready for connections.
forked process: 605990
child process started successfully, parent exiting
Current time: 20250426_181732, num_cpus[16], num_gpus[1], num_train_samples[10000], batch_size[16], epoches[10]
INFO:__main__:Initializing Spark...
INFO:__main__:16 cores for spark
:: loading settings :: url = jar:file:/home/yzhengbs/anaconda3/envs/5003/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /home/yzhengbs/.ivy2/cache
The jars for the packages stored in: /home/yzhengbs/.ivy2/jars
org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-e92311fd-9b1c-4260-8d09-91a71301e40f;1.0
	confs: [default]
	found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
	found org.mongodb#mongodb-driver-sync;4.0.5 in central
	found org.mongodb#bson;4.0.5 in central
	found org.mongodb#mongodb-driver-core;4.0.5 in central
:: resolution report :: resolve 161ms :: artifacts dl 15ms
	:: modules in use:
	org.mongodb#bson;4.0.5 from central in [default]
	org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
	org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
	org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-e92311fd-9b1c-4260-8d09-91a71301e40f
	confs: [default]
	0 artifacts copied, 4 already retrieved (0kB/8ms)
25/04/26 10:17:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
INFO:__main__:No cached Parquet files found. Running full pipeline...
INFO:__main__:Loading data to MongoDB...
INFO:__main__:Loading IMDB dataset...
INFO:__main__:Loading SST-2 dataset...
INFO:__main__:Writing datasets to MongoDB...
25/04/26 10:18:08 WARN TaskSetManager: Stage 0 contains a task of very large size (3984 KiB). The maximum recommended task size is 1000 KiB.
[Stage 0:>                                                        (0 + 16) / 16][Stage 0:===>                                                     (1 + 15) / 16][Stage 0:=====================>                                   (6 + 10) / 16]                                                                                INFO:__main__:Data loading to MongoDB took 24.22 seconds
INFO:__main__:Distributed preprocessing and saving to Parquet...
INFO:__main__:Reading data from MongoDB...
INFO:__main__:Applying text preprocessing (lowercase, remove punctuation, non-alnum, continuous whitespace, strip)...
INFO:__main__:Tokenizing data...
INFO:__main__:num_samples[10000]
INFO:__main__:num_partitions 16
INFO:__main__:Writing Parquet files: train=processed_data/train_dff4c14733ae4ba7bd2419440a979ee0, test=processed_data/test_04c4dde643844bd584202a899866fa19, sst2=processed_data/sst2_2e62ae329d4e4ba6a0e377b399f9cbd7
25/04/26 10:18:11 WARN TaskSetManager: Stage 2 contains a task of very large size (3984 KiB). The maximum recommended task size is 1000 KiB.
[Stage 2:>                                                        (0 + 16) / 32][Stage 2:=======>                                                 (4 + 16) / 32][Stage 2:============>                                            (7 + 16) / 32][Stage 2:===================>                                    (11 + 16) / 32][Stage 2:=====================>                                  (12 + 16) / 32][Stage 2:======================>                                 (13 + 16) / 32][Stage 2:========================>                               (14 + 16) / 32][Stage 2:==========================>                             (15 + 16) / 32][Stage 2:============================>                           (16 + 16) / 32][Stage 2:=============================>                          (17 + 15) / 32][Stage 2:===============================>                        (18 + 14) / 32][Stage 2:===================================>                    (20 + 12) / 32][Stage 2:======================================>                 (22 + 10) / 32][Stage 2:==========================================>              (24 + 8) / 32][Stage 2:==============================================>          (26 + 6) / 32][Stage 2:================================================>        (27 + 5) / 32][Stage 2:===================================================>     (29 + 3) / 32][Stage 2:=====================================================>   (30 + 2) / 32][Stage 2:=======================================================> (31 + 1) / 32][Stage 7:>                                                        (0 + 16) / 16]                                                                                INFO:__main__:30.6509s for train_df partition
25/04/26 10:18:42 WARN TaskSetManager: Stage 8 contains a task of very large size (3984 KiB). The maximum recommended task size is 1000 KiB.
[Stage 8:>                                                        (0 + 16) / 32][Stage 8:==========>                                              (6 + 16) / 32][Stage 8:=====================>                                  (12 + 16) / 32][Stage 8:===============================>                        (18 + 14) / 32][Stage 8:=================================>                      (19 + 13) / 32][Stage 8:===================================>                    (20 + 12) / 32][Stage 8:====================================>                   (21 + 11) / 32][Stage 8:======================================>                 (22 + 10) / 32][Stage 8:==========================================>              (24 + 8) / 32][Stage 8:=================================================>       (28 + 4) / 32][Stage 8:===================================================>     (29 + 3) / 32][Stage 8:=====================================================>   (30 + 2) / 32]                                                                                INFO:__main__:17.3536s for test_df partition
25/04/26 10:18:59 WARN TaskSetManager: Stage 14 contains a task of very large size (3984 KiB). The maximum recommended task size is 1000 KiB.
[Stage 14:=========================>                             (15 + 16) / 32][Stage 14:==================================>                    (20 + 12) / 32][Stage 14:==========================================>             (24 + 8) / 32][Stage 14:=================================================>      (28 + 4) / 32][Stage 14:====================================================>   (30 + 2) / 32][Stage 14:======================================================> (31 + 1) / 32]                                                                                INFO:__main__:8.1948s for sst2_df partition
INFO:__main__:Distributed preprocessing took 56.38 seconds
INFO:__main__:Using 1 GPU(s)
INFO:__main__:Distributed fine-tuning...
INFO:__mp_main__:Rank 0: Assigned 16 files: ['processed_data/train_dff4c14733ae4ba7bd2419440a979ee0/part-00000-24a40d8d-4c3d-4668-9a04-6a48cf2d263e-c000.snappy.parquet', 'processed_data/train_dff4c14733ae4ba7bd2419440a979ee0/part-00001-24a40d8d-4c3d-4668-9a04-6a48cf2d263e-c000.snappy.parquet', 'processed_data/train_dff4c14733ae4ba7bd2419440a979ee0/part-00002-24a40d8d-4c3d-4668-9a04-6a48cf2d263e-c000.snappy.parquet', 'processed_data/train_dff4c14733ae4ba7bd2419440a979ee0/part-00003-24a40d8d-4c3d-4668-9a04-6a48cf2d263e-c000.snappy.parquet', 'processed_data/train_dff4c14733ae4ba7bd2419440a979ee0/part-00004-24a40d8d-4c3d-4668-9a04-6a48cf2d263e-c000.snappy.parquet', 'processed_data/train_dff4c14733ae4ba7bd2419440a979ee0/part-00005-24a40d8d-4c3d-4668-9a04-6a48cf2d263e-c000.snappy.parquet', 'processed_data/train_dff4c14733ae4ba7bd2419440a979ee0/part-00006-24a40d8d-4c3d-4668-9a04-6a48cf2d263e-c000.snappy.parquet', 'processed_data/train_dff4c14733ae4ba7bd2419440a979ee0/part-00007-24a40d8d-4c3d-4668-9a04-6a48cf2d263e-c000.snappy.parquet', 'processed_data/train_dff4c14733ae4ba7bd2419440a979ee0/part-00008-24a40d8d-4c3d-4668-9a04-6a48cf2d263e-c000.snappy.parquet', 'processed_data/train_dff4c14733ae4ba7bd2419440a979ee0/part-00009-24a40d8d-4c3d-4668-9a04-6a48cf2d263e-c000.snappy.parquet', 'processed_data/train_dff4c14733ae4ba7bd2419440a979ee0/part-00010-24a40d8d-4c3d-4668-9a04-6a48cf2d263e-c000.snappy.parquet', 'processed_data/train_dff4c14733ae4ba7bd2419440a979ee0/part-00011-24a40d8d-4c3d-4668-9a04-6a48cf2d263e-c000.snappy.parquet', 'processed_data/train_dff4c14733ae4ba7bd2419440a979ee0/part-00012-24a40d8d-4c3d-4668-9a04-6a48cf2d263e-c000.snappy.parquet', 'processed_data/train_dff4c14733ae4ba7bd2419440a979ee0/part-00013-24a40d8d-4c3d-4668-9a04-6a48cf2d263e-c000.snappy.parquet', 'processed_data/train_dff4c14733ae4ba7bd2419440a979ee0/part-00014-24a40d8d-4c3d-4668-9a04-6a48cf2d263e-c000.snappy.parquet', 'processed_data/train_dff4c14733ae4ba7bd2419440a979ee0/part-00015-24a40d8d-4c3d-4668-9a04-6a48cf2d263e-c000.snappy.parquet']
INFO:__mp_main__:Rank 0: Assigned 16 files: ['processed_data/test_04c4dde643844bd584202a899866fa19/part-00000-f5960f33-2187-4a0b-a3ed-327e96edda02-c000.snappy.parquet', 'processed_data/test_04c4dde643844bd584202a899866fa19/part-00001-f5960f33-2187-4a0b-a3ed-327e96edda02-c000.snappy.parquet', 'processed_data/test_04c4dde643844bd584202a899866fa19/part-00002-f5960f33-2187-4a0b-a3ed-327e96edda02-c000.snappy.parquet', 'processed_data/test_04c4dde643844bd584202a899866fa19/part-00003-f5960f33-2187-4a0b-a3ed-327e96edda02-c000.snappy.parquet', 'processed_data/test_04c4dde643844bd584202a899866fa19/part-00004-f5960f33-2187-4a0b-a3ed-327e96edda02-c000.snappy.parquet', 'processed_data/test_04c4dde643844bd584202a899866fa19/part-00005-f5960f33-2187-4a0b-a3ed-327e96edda02-c000.snappy.parquet', 'processed_data/test_04c4dde643844bd584202a899866fa19/part-00006-f5960f33-2187-4a0b-a3ed-327e96edda02-c000.snappy.parquet', 'processed_data/test_04c4dde643844bd584202a899866fa19/part-00007-f5960f33-2187-4a0b-a3ed-327e96edda02-c000.snappy.parquet', 'processed_data/test_04c4dde643844bd584202a899866fa19/part-00008-f5960f33-2187-4a0b-a3ed-327e96edda02-c000.snappy.parquet', 'processed_data/test_04c4dde643844bd584202a899866fa19/part-00009-f5960f33-2187-4a0b-a3ed-327e96edda02-c000.snappy.parquet', 'processed_data/test_04c4dde643844bd584202a899866fa19/part-00010-f5960f33-2187-4a0b-a3ed-327e96edda02-c000.snappy.parquet', 'processed_data/test_04c4dde643844bd584202a899866fa19/part-00011-f5960f33-2187-4a0b-a3ed-327e96edda02-c000.snappy.parquet', 'processed_data/test_04c4dde643844bd584202a899866fa19/part-00012-f5960f33-2187-4a0b-a3ed-327e96edda02-c000.snappy.parquet', 'processed_data/test_04c4dde643844bd584202a899866fa19/part-00013-f5960f33-2187-4a0b-a3ed-327e96edda02-c000.snappy.parquet', 'processed_data/test_04c4dde643844bd584202a899866fa19/part-00014-f5960f33-2187-4a0b-a3ed-327e96edda02-c000.snappy.parquet', 'processed_data/test_04c4dde643844bd584202a899866fa19/part-00015-f5960f33-2187-4a0b-a3ed-327e96edda02-c000.snappy.parquet']
INFO:__mp_main__:Rank 0: Assigned 16 files: ['processed_data/sst2_2e62ae329d4e4ba6a0e377b399f9cbd7/part-00000-ce20f261-52de-4da3-85be-e13e40896ddb-c000.snappy.parquet', 'processed_data/sst2_2e62ae329d4e4ba6a0e377b399f9cbd7/part-00001-ce20f261-52de-4da3-85be-e13e40896ddb-c000.snappy.parquet', 'processed_data/sst2_2e62ae329d4e4ba6a0e377b399f9cbd7/part-00002-ce20f261-52de-4da3-85be-e13e40896ddb-c000.snappy.parquet', 'processed_data/sst2_2e62ae329d4e4ba6a0e377b399f9cbd7/part-00003-ce20f261-52de-4da3-85be-e13e40896ddb-c000.snappy.parquet', 'processed_data/sst2_2e62ae329d4e4ba6a0e377b399f9cbd7/part-00004-ce20f261-52de-4da3-85be-e13e40896ddb-c000.snappy.parquet', 'processed_data/sst2_2e62ae329d4e4ba6a0e377b399f9cbd7/part-00005-ce20f261-52de-4da3-85be-e13e40896ddb-c000.snappy.parquet', 'processed_data/sst2_2e62ae329d4e4ba6a0e377b399f9cbd7/part-00006-ce20f261-52de-4da3-85be-e13e40896ddb-c000.snappy.parquet', 'processed_data/sst2_2e62ae329d4e4ba6a0e377b399f9cbd7/part-00007-ce20f261-52de-4da3-85be-e13e40896ddb-c000.snappy.parquet', 'processed_data/sst2_2e62ae329d4e4ba6a0e377b399f9cbd7/part-00008-ce20f261-52de-4da3-85be-e13e40896ddb-c000.snappy.parquet', 'processed_data/sst2_2e62ae329d4e4ba6a0e377b399f9cbd7/part-00009-ce20f261-52de-4da3-85be-e13e40896ddb-c000.snappy.parquet', 'processed_data/sst2_2e62ae329d4e4ba6a0e377b399f9cbd7/part-00010-ce20f261-52de-4da3-85be-e13e40896ddb-c000.snappy.parquet', 'processed_data/sst2_2e62ae329d4e4ba6a0e377b399f9cbd7/part-00011-ce20f261-52de-4da3-85be-e13e40896ddb-c000.snappy.parquet', 'processed_data/sst2_2e62ae329d4e4ba6a0e377b399f9cbd7/part-00012-ce20f261-52de-4da3-85be-e13e40896ddb-c000.snappy.parquet', 'processed_data/sst2_2e62ae329d4e4ba6a0e377b399f9cbd7/part-00013-ce20f261-52de-4da3-85be-e13e40896ddb-c000.snappy.parquet', 'processed_data/sst2_2e62ae329d4e4ba6a0e377b399f9cbd7/part-00014-ce20f261-52de-4da3-85be-e13e40896ddb-c000.snappy.parquet', 'processed_data/sst2_2e62ae329d4e4ba6a0e377b399f9cbd7/part-00015-ce20f261-52de-4da3-85be-e13e40896ddb-c000.snappy.parquet']
INFO:__mp_main__:Rank 0: Local train batch count = 625, Global min train batch count = 625
INFO:__mp_main__:Rank 0: Local test batch count = 125, Global min test batch count = 125
INFO:__mp_main__:Rank 0: Local sst2_test batch count = 125, Global min sst2_test batch count = 125
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/yzhengbs/anaconda3/envs/5003/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:444: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.
  warnings.warn(
INFO:__mp_main__:Epoch 1, Avg Training Loss: 0.5864
INFO:__mp_main__:GPU[0], Epoch 1, Avg Loss: 0.5864
INFO:__mp_main__:Epoch 1, IMDB Test Eval Loss: 0.3892, Accuracy: 0.8415
INFO:__mp_main__:Epoch 1, SST-2 Test Eval Loss: 0.6438, Accuracy: 0.6320
INFO:__mp_main__:Epoch 2, Avg Training Loss: 0.4242
INFO:__mp_main__:GPU[0], Epoch 2, Avg Loss: 0.4242
INFO:__mp_main__:Epoch 2, IMDB Test Eval Loss: 0.3112, Accuracy: 0.8830
INFO:__mp_main__:Epoch 2, SST-2 Test Eval Loss: 0.6119, Accuracy: 0.6805
INFO:__mp_main__:Epoch 3, Avg Training Loss: 0.3838
INFO:__mp_main__:GPU[0], Epoch 3, Avg Loss: 0.3838
INFO:__mp_main__:Epoch 3, IMDB Test Eval Loss: 0.3052, Accuracy: 0.8865
INFO:__mp_main__:Epoch 3, SST-2 Test Eval Loss: 0.6162, Accuracy: 0.6950
INFO:__mp_main__:Epoch 4, Avg Training Loss: 0.3763
INFO:__mp_main__:GPU[0], Epoch 4, Avg Loss: 0.3763
INFO:__mp_main__:Epoch 4, IMDB Test Eval Loss: 0.2960, Accuracy: 0.8895
INFO:__mp_main__:Epoch 4, SST-2 Test Eval Loss: 0.5994, Accuracy: 0.7050
INFO:__mp_main__:Epoch 5, Avg Training Loss: 0.3596
INFO:__mp_main__:GPU[0], Epoch 5, Avg Loss: 0.3596
INFO:__mp_main__:Epoch 5, IMDB Test Eval Loss: 0.2951, Accuracy: 0.8925
INFO:__mp_main__:Epoch 5, SST-2 Test Eval Loss: 0.5669, Accuracy: 0.7315
INFO:__mp_main__:Epoch 6, Avg Training Loss: 0.3567
INFO:__mp_main__:GPU[0], Epoch 6, Avg Loss: 0.3567
INFO:__mp_main__:Epoch 6, IMDB Test Eval Loss: 0.2872, Accuracy: 0.8945
INFO:__mp_main__:Epoch 6, SST-2 Test Eval Loss: 0.5768, Accuracy: 0.7330
INFO:__mp_main__:Epoch 7, Avg Training Loss: 0.3431
INFO:__mp_main__:GPU[0], Epoch 7, Avg Loss: 0.3431
INFO:__mp_main__:Epoch 7, IMDB Test Eval Loss: 0.2975, Accuracy: 0.8950
INFO:__mp_main__:Epoch 7, SST-2 Test Eval Loss: 0.5633, Accuracy: 0.7340
INFO:__mp_main__:Epoch 8, Avg Training Loss: 0.3408
INFO:__mp_main__:GPU[0], Epoch 8, Avg Loss: 0.3408
INFO:__mp_main__:Epoch 8, IMDB Test Eval Loss: 0.2904, Accuracy: 0.8975
INFO:__mp_main__:Epoch 8, SST-2 Test Eval Loss: 0.5704, Accuracy: 0.7365
INFO:__mp_main__:Epoch 9, Avg Training Loss: 0.3397
INFO:__mp_main__:GPU[0], Epoch 9, Avg Loss: 0.3397
INFO:__mp_main__:Epoch 9, IMDB Test Eval Loss: 0.3024, Accuracy: 0.8940
INFO:__mp_main__:Epoch 9, SST-2 Test Eval Loss: 0.5679, Accuracy: 0.7450
INFO:__mp_main__:Epoch 10, Avg Training Loss: 0.3329
INFO:__mp_main__:GPU[0], Epoch 10, Avg Loss: 0.3329
INFO:__mp_main__:Epoch 10, IMDB Test Eval Loss: 0.2928, Accuracy: 0.8935
INFO:__mp_main__:Epoch 10, SST-2 Test Eval Loss: 0.5723, Accuracy: 0.7470
INFO:__mp_main__:Training wall time (max across ranks): 225.19 seconds
/home/yzhengbs/anaconda3/envs/5003/lib/python3.9/site-packages/torch/distributed/fsdp/_state_dict_utils.py:768: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.
  warnings.warn(
/home/yzhengbs/anaconda3/envs/5003/lib/python3.9/site-packages/torch/distributed/fsdp/_state_dict_utils.py:711: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.
  warnings.warn(
INFO:__mp_main__:Checkpoint saved at checkpoints/20250426_182302_bert_finetuned_epoch_10.pt
INFO:__mp_main__:Rank 0: IMDB Test local batch count = 125, global min = 125
INFO:__mp_main__:IMDB Test Accuracy: 0.894
INFO:__mp_main__:GPU[0]: IMDB Test Accuracy: 0.8940
INFO:__mp_main__:Rank 0: SST-2 Test local batch count = 125, global min = 125
INFO:__mp_main__:SST-2 Test Accuracy: 0.747
INFO:__mp_main__:GPU[0]: SST-2 Test Accuracy: 0.7470
/home/yzhengbs/vinc/5003/project/train.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(checkpoint_path)
INFO:__main__:Loss curves saved at plots/loss_curves_20250426_182310.png
INFO:__main__:2025/04/26-18:23:10	NUM_CPUs[16]		NUM_GPUs[1]		NUM_TRAIN_SAMPLES[10000]		preprocess_time[56.38 sec]		finetune_time[225.19 sec]

INFO:py4j.clientserver:Closing down clientserver connection
