INFO:__main__:Initializing Spark...
INFO:__main__:14 cores for spark
25/04/26 21:17:03 WARN Utils: Your hostname, yPC resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)
25/04/26 21:17:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
:: loading settings :: url = jar:file:/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /home/goodh/.ivy2/cache
The jars for the packages stored in: /home/goodh/.ivy2/jars
org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-0ced5ac7-4419-4597-9b5b-f090476341b2;1.0
	confs: [default]
	found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central
	found org.mongodb#mongodb-driver-sync;4.0.5 in central
	found org.mongodb#bson;4.0.5 in central
	found org.mongodb#mongodb-driver-core;4.0.5 in central
:: resolution report :: resolve 89ms :: artifacts dl 3ms
	:: modules in use:
	org.mongodb#bson;4.0.5 from central in [default]
	org.mongodb#mongodb-driver-core;4.0.5 from central in [default]
	org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]
	org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-0ced5ac7-4419-4597-9b5b-f090476341b2
	confs: [default]
	0 artifacts copied, 4 already retrieved (0kB/3ms)
25/04/26 21:17:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/04/26 21:17:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
INFO:__main__:No cached Parquet files found. Running full pipeline...
INFO:__main__:Loading data to MongoDB...
INFO:__main__:Loading IMDB dataset...
INFO:__main__:Loading SST-2 dataset...
INFO:__main__:Writing datasets to MongoDB...
25/04/26 21:17:33 WARN TaskSetManager: Stage 0 contains a task of very large size (3984 KiB). The maximum recommended task size is 1000 KiB.
[Stage 0:>                                                        (0 + 14) / 14][Stage 0:========================>                                 (6 + 8) / 14]                                                                                INFO:__main__:Data loading to MongoDB took 31.09 seconds
INFO:__main__:Distributed preprocessing and saving to Parquet...
INFO:__main__:Reading data from MongoDB...
INFO:__main__:Applying text preprocessing (lowercase, remove punctuation, non-alnum, continuous whitespace, strip)...
INFO:__main__:Tokenizing data...
INFO:__main__:num_samples[40000]
INFO:__main__:num_partitions 14
INFO:__main__:Writing Parquet files: train=processed_data/train_ca4a43cef3534529aa96b6ab64da6a90, test=processed_data/test_94b549d87f3947acba5623618bc31895, sst2=processed_data/sst2_8bf6cec6f0d440ad8a44399e01ab5742
25/04/26 21:17:36 WARN TaskSetManager: Stage 2 contains a task of very large size (3984 KiB). The maximum recommended task size is 1000 KiB.
[Stage 2:>                                                        (0 + 14) / 28][Stage 2:==>                                                      (1 + 14) / 28][Stage 2:====>                                                    (2 + 14) / 28][Stage 2:==========>                                              (5 + 14) / 28][Stage 2:==============>                                          (7 + 14) / 28][Stage 2:================>                                        (8 + 14) / 28][Stage 2:====================>                                   (10 + 14) / 28][Stage 2:==============================>                         (15 + 13) / 28][Stage 2:================================>                       (16 + 12) / 28][Stage 2:==================================>                     (17 + 11) / 28][Stage 2:====================================>                   (18 + 10) / 28][Stage 2:======================================>                  (19 + 9) / 28][Stage 2:==============================================>          (23 + 5) / 28][Stage 2:================================================>        (24 + 4) / 28][Stage 2:====================================================>    (26 + 2) / 28][Stage 7:>                                                        (0 + 14) / 14]                                                                                INFO:__main__:27.1577s for train_df partition
25/04/26 21:18:03 WARN TaskSetManager: Stage 8 contains a task of very large size (3984 KiB). The maximum recommended task size is 1000 KiB.
[Stage 8:>                                                        (0 + 14) / 28][Stage 8:========>                                                (4 + 14) / 28][Stage 8:==============>                                          (7 + 14) / 28][Stage 8:======================>                                 (11 + 14) / 28][Stage 8:==========================>                             (13 + 14) / 28][Stage 8:================================>                       (16 + 12) / 28][Stage 8:==================================>                     (17 + 11) / 28][Stage 8:====================================>                   (18 + 10) / 28][Stage 8:======================================>                  (19 + 9) / 28][Stage 8:========================================>                (20 + 8) / 28][Stage 8:============================================>            (22 + 6) / 28][Stage 8:================================================>        (24 + 4) / 28][Stage 8:==================================================>      (25 + 3) / 28][Stage 8:======================================================>  (27 + 1) / 28]                                                                                INFO:__main__:18.7004s for test_df partition
25/04/26 21:18:22 WARN TaskSetManager: Stage 14 contains a task of very large size (3984 KiB). The maximum recommended task size is 1000 KiB.
[Stage 14:===========================>                           (14 + 14) / 28][Stage 14:===============================>                       (16 + 12) / 28][Stage 14:============================================>           (22 + 6) / 28][Stage 14:====================================================>   (26 + 2) / 28][Stage 14:======================================================> (27 + 1) / 28]                                                                                INFO:__main__:7.4790s for sst2_df partition
INFO:__main__:Distributed preprocessing took 53.53 seconds
INFO:__main__:Using 1 GPU(s)
INFO:__main__:Distributed fine-tuning...
INFO:__mp_main__:Rank 0: Local train batch count = 2500, Global min train batch count = 2500
INFO:__mp_main__:Rank 0: Local test batch count = 500, Global min test batch count = 500
INFO:__mp_main__:Rank 0: Local sst2_test batch count = 500, Global min sst2_test batch count = 500
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/torch/distributed/fsdp/_init_utils.py:444: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.
  warnings.warn(
INFO:__mp_main__:Epoch 1, Avg Training Loss: 0.4385
INFO:__mp_main__:GPU[0], Epoch 1, Avg Loss: 0.4385
INFO:__mp_main__:Epoch 1, IMDB Test Eval Loss: 0.2957, Accuracy: 0.8882
INFO:__mp_main__:Epoch 1, SST-2 Test Eval Loss: 0.4124, Accuracy: 0.8135
INFO:__mp_main__:Epoch 2, Avg Training Loss: 0.3599
INFO:__mp_main__:GPU[0], Epoch 2, Avg Loss: 0.3599
INFO:__mp_main__:Epoch 2, IMDB Test Eval Loss: 0.3081, Accuracy: 0.8896
INFO:__mp_main__:Epoch 2, SST-2 Test Eval Loss: 0.3993, Accuracy: 0.8255
INFO:__mp_main__:Epoch 3, Avg Training Loss: 0.3464
INFO:__mp_main__:GPU[0], Epoch 3, Avg Loss: 0.3464
INFO:__mp_main__:Epoch 3, IMDB Test Eval Loss: 0.2995, Accuracy: 0.8902
INFO:__mp_main__:Epoch 3, SST-2 Test Eval Loss: 0.3982, Accuracy: 0.8284
INFO:__mp_main__:Epoch 4, Avg Training Loss: 0.3352
INFO:__mp_main__:GPU[0], Epoch 4, Avg Loss: 0.3352
INFO:__mp_main__:Epoch 4, IMDB Test Eval Loss: 0.2930, Accuracy: 0.8960
INFO:__mp_main__:Epoch 4, SST-2 Test Eval Loss: 0.3994, Accuracy: 0.8266
INFO:__mp_main__:Epoch 5, Avg Training Loss: 0.3242
INFO:__mp_main__:GPU[0], Epoch 5, Avg Loss: 0.3242
INFO:__mp_main__:Epoch 5, IMDB Test Eval Loss: 0.2863, Accuracy: 0.8970
INFO:__mp_main__:Epoch 5, SST-2 Test Eval Loss: 0.4008, Accuracy: 0.8249
INFO:__mp_main__:Epoch 6, Avg Training Loss: 0.3244
INFO:__mp_main__:GPU[0], Epoch 6, Avg Loss: 0.3244
INFO:__mp_main__:Epoch 6, IMDB Test Eval Loss: 0.2768, Accuracy: 0.9016
INFO:__mp_main__:Epoch 6, SST-2 Test Eval Loss: 0.3961, Accuracy: 0.8261
INFO:__mp_main__:Epoch 7, Avg Training Loss: 0.3165
INFO:__mp_main__:GPU[0], Epoch 7, Avg Loss: 0.3165
INFO:__mp_main__:Epoch 7, IMDB Test Eval Loss: 0.2775, Accuracy: 0.8989
INFO:__mp_main__:Epoch 7, SST-2 Test Eval Loss: 0.3979, Accuracy: 0.8263
INFO:__mp_main__:Epoch 8, Avg Training Loss: 0.3119
INFO:__mp_main__:GPU[0], Epoch 8, Avg Loss: 0.3119
INFO:__mp_main__:Epoch 8, IMDB Test Eval Loss: 0.2765, Accuracy: 0.8999
INFO:__mp_main__:Epoch 8, SST-2 Test Eval Loss: 0.3946, Accuracy: 0.8284
INFO:__mp_main__:Epoch 9, Avg Training Loss: 0.3072
INFO:__mp_main__:GPU[0], Epoch 9, Avg Loss: 0.3072
INFO:__mp_main__:Epoch 9, IMDB Test Eval Loss: 0.2705, Accuracy: 0.9040
INFO:__mp_main__:Epoch 9, SST-2 Test Eval Loss: 0.4028, Accuracy: 0.8210
INFO:__mp_main__:Epoch 10, Avg Training Loss: 0.3048
INFO:__mp_main__:GPU[0], Epoch 10, Avg Loss: 0.3048
INFO:__mp_main__:Epoch 10, IMDB Test Eval Loss: 0.2551, Accuracy: 0.9062
INFO:__mp_main__:Epoch 10, SST-2 Test Eval Loss: 0.3933, Accuracy: 0.8274
INFO:__mp_main__:Training wall time (max across ranks): 1083.42 seconds
/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/torch/distributed/fsdp/_state_dict_utils.py:773: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.
  warnings.warn(
/home/goodh/miniconda3/envs/5003/lib/python3.9/site-packages/torch/distributed/fsdp/_state_dict_utils.py:711: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.
  warnings.warn(
INFO:__mp_main__:Checkpoint saved at checkpoints/20250426_213640_bert_finetuned_epoch_10.pt
INFO:__mp_main__:IMDB Test Accuracy: 0.906375
INFO:__mp_main__:GPU[0]: IMDB Test Accuracy: 0.9064
INFO:__mp_main__:SST-2 Test Accuracy: 0.8275
INFO:__mp_main__:GPU[0]: SST-2 Test Accuracy: 0.8275
INFO:__main__:Loss curves saved at plots/loss_curves_20250426_213656.png
INFO:__main__:2025/04/26-21:36:56	NUM_CPUs[14]		NUM_GPUs[1]		NUM_TRAIN_SAMPLES[40000]		preprocess_time[53.53 sec]		finetune_time[1083.42 sec]

INFO:py4j.clientserver:Closing down clientserver connection
